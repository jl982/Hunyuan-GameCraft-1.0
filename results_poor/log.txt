ubuntu@ip-172-31-1-60:~$ df -h
Filesystem                      Size  Used Avail Use% Mounted on
/dev/root                        97G   21G   76G  22% /
tmpfs                            31G   80K   31G   1% /dev/shm
tmpfs                            13G  1.1M   13G   1% /run
tmpfs                           5.0M     0  5.0M   0% /run/lock
efivarfs                        128K  3.8K  120K   4% /sys/firmware/efi/efivars
/dev/nvme0n1p15                 105M  6.1M   99M   6% /boot/efi
/dev/mapper/vg.01-lv_ephemeral  412G   28K  391G   1% /opt/dlami/nvme
tmpfs                           6.2G  4.0K  6.2G   1% /run/user/1000
ubuntu@ip-172-31-1-60:~$ cd /opt/dlami/nvme/
ubuntu@ip-172-31-1-60:/opt/dlami/nvme$ git clone https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0.git
cd Hunyuan-GameCraft-1.0
Cloning into 'Hunyuan-GameCraft-1.0'...
remote: Enumerating objects: 173, done.
remote: Counting objects: 100% (50/50), done.
remote: Compressing objects: 100% (24/24), done.
remote: Total 173 (delta 34), reused 28 (delta 26), pack-reused 123 (from 1)
Receiving objects: 100% (173/173), 29.68 MiB | 42.10 MiB/s, done.
Resolving deltas: 100% (68/68), done.
ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ conda
conda: command not found
ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ source ~/miniconda3/bin/activate
(base) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ conda init --all
no change     /home/ubuntu/miniconda3/condabin/conda
no change     /home/ubuntu/miniconda3/bin/conda
no change     /home/ubuntu/miniconda3/bin/conda-env
no change     /home/ubuntu/miniconda3/bin/activate
no change     /home/ubuntu/miniconda3/bin/deactivate
no change     /home/ubuntu/miniconda3/etc/profile.d/conda.sh
no change     /home/ubuntu/miniconda3/etc/fish/conf.d/conda.fish
no change     /home/ubuntu/miniconda3/shell/condabin/Conda.psm1
no change     /home/ubuntu/miniconda3/shell/condabin/conda-hook.ps1
no change     /home/ubuntu/miniconda3/lib/python3.13/site-packages/xontrib/conda.xsh
no change     /home/ubuntu/miniconda3/etc/profile.d/conda.csh
modified      /home/ubuntu/.bashrc
modified      /home/ubuntu/.zshrc
modified      /home/ubuntu/.config/fish/config.fish
modified      /home/ubuntu/.xonshrc
modified      /home/ubuntu/.tcshrc

==> For changes to take effect, close and re-open your current shell. <==

(base) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ conda
usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...

conda is a tool for managing and deploying applications, environments and packages.

options:
  -h, --help            Show this help message and exit.
  -v, --verbose         Can be used multiple times. Once for detailed output, twice for INFO logging, thrice for DEBUG logging, four times for TRACE logging.
  --no-plugins          Disable all plugins that are not built into conda.
  -V, --version         Show the conda version number and exit.

commands:
  The following built-in and plugins subcommands are available.

  COMMAND
    activate            Activate a conda environment.
    clean               Remove unused packages and caches.
    commands            List all available conda subcommands (including those from plugins). Generally only used by tab-completion.
    compare             Compare packages between conda environments.
    config              Modify configuration values in .condarc.
    content-trust       Signing and verification tools for Conda
    create              Create a new conda environment from a list of specified packages.
    deactivate          Deactivate the current active conda environment.
    doctor              Display a health report for your environment.
    env                 Create and manage conda environments.
    export              Export a given environment
    info                Display information about current conda install.
    init                Initialize conda for shell interaction.
    install             Install a list of packages into a specified conda environment.
    list                List installed packages in a conda environment.
    notices             Retrieve latest channel notifications.
    package             Create low-level conda packages. (EXPERIMENTAL)
    remove (uninstall)  Remove a list of packages from a specified conda environment.
    rename              Rename an existing environment.
    repoquery           Advanced search for repodata.
    run                 Run an executable in a conda environment.
    search              Search for packages and display associated information using the MatchSpec format.
    token               See `conda token --help`.
    tos                 A subcommand for viewing, accepting, rejecting, and otherwise interacting with a channel's Terms of Service (ToS). This plugin periodically checks for updated Terms of Service for the active/selected channels. Channels with a Terms of Service will need to
                        be accepted or rejected prior to use. Conda will only allow package installation from channels without a Terms of Service or with an accepted Terms of Service. Attempting to use a channel with a rejected Terms of Service will result in an error.
    update (upgrade)    Update conda packages to the latest compatible version.
(base) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ conda create -n HYGameCraft python==3.10
Do you accept the Terms of Service (ToS) for https://repo.anaconda.com/pkgs/main? [(a)ccept/(r)eject/(v)iew]: y
Please select one of the available options
Do you accept the Terms of Service (ToS) for https://repo.anaconda.com/pkgs/main? [(a)ccept/(r)eject/(v)iew]: a
Do you accept the Terms of Service (ToS) for https://repo.anaconda.com/pkgs/r? [(a)ccept/(r)eject/(v)iew]: a
2 channel Terms of Service accepted
Retrieving notices: done
Channels:
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
    current version: 25.7.0
    latest version: 25.9.1

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/ubuntu/miniconda3/envs/HYGameCraft

  added / updated specs:
    - python==3.10


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    ca-certificates-2025.9.9   |       h06a4308_0         127 KB
    ld_impl_linux-64-2.44      |       h153f514_2         672 KB
    libffi-3.3                 |       he6710b0_2          50 KB
    libzlib-1.3.1              |       hb25bd0a_0          59 KB
    openssl-1.1.1w             |       h7f8727e_0         3.7 MB
    pip-25.2                   |     pyhc872135_1         1.1 MB
    python-3.10.0              |       h12debd9_5        23.5 MB
    setuptools-80.9.0          |  py310h06a4308_0         1.4 MB
    wheel-0.45.1               |  py310h06a4308_0         115 KB
    zlib-1.3.1                 |       hb25bd0a_0          96 KB
    ------------------------------------------------------------
                                           Total:        30.9 MB

The following NEW packages will be INSTALLED:

  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main
  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu
  bzip2              pkgs/main/linux-64::bzip2-1.0.8-h5eee18b_6
  ca-certificates    pkgs/main/linux-64::ca-certificates-2025.9.9-h06a4308_0
  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.44-h153f514_2
  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2
  libgcc-ng          pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1
  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1
  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1
  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0
  libxcb             pkgs/main/linux-64::libxcb-1.17.0-h9b100fa_0
  libzlib            pkgs/main/linux-64::libzlib-1.3.1-hb25bd0a_0
  ncurses            pkgs/main/linux-64::ncurses-6.5-h7934f7d_0
  openssl            pkgs/main/linux-64::openssl-1.1.1w-h7f8727e_0
  pip                pkgs/main/noarch::pip-25.2-pyhc872135_1
  pthread-stubs      pkgs/main/linux-64::pthread-stubs-0.3-h0ce48e5_1
  python             pkgs/main/linux-64::python-3.10.0-h12debd9_5
  readline           pkgs/main/linux-64::readline-8.3-hc2a1206_0
  setuptools         pkgs/main/linux-64::setuptools-80.9.0-py310h06a4308_0
  sqlite             pkgs/main/linux-64::sqlite-3.50.2-hb25bd0a_1
  tk                 pkgs/main/linux-64::tk-8.6.15-h54e0aa7_0
  tzdata             pkgs/main/noarch::tzdata-2025b-h04d1e81_0
  wheel              pkgs/main/linux-64::wheel-0.45.1-py310h06a4308_0
  xorg-libx11        pkgs/main/linux-64::xorg-libx11-1.8.12-h9b100fa_1
  xorg-libxau        pkgs/main/linux-64::xorg-libxau-1.0.12-h9b100fa_0
  xorg-libxdmcp      pkgs/main/linux-64::xorg-libxdmcp-1.1.5-h9b100fa_0
  xorg-xorgproto     pkgs/main/linux-64::xorg-xorgproto-2024.1-h5eee18b_1
  xz                 pkgs/main/linux-64::xz-5.6.4-h5eee18b_1
  zlib               pkgs/main/linux-64::zlib-1.3.1-hb25bd0a_0


Proceed ([y]/n)?


Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
#
# To activate this environment, use
#
#     $ conda activate HYGameCraft
#
# To deactivate an active environment, use
#
#     $ conda deactivate

(base) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ conda activate HYGameCraft
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=12.4 -c pytorch -c nvidia
2 channel Terms of Service accepted
Channels:
 - pytorch
 - nvidia
 - defaults
Platform: linux-64
Collecting package metadata (repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
    current version: 25.7.0
    latest version: 25.9.1

Please update conda by running

    $ conda update -n base -c defaults conda



## Package Plan ##

  environment location: /home/ubuntu/miniconda3/envs/HYGameCraft

  added / updated specs:
    - pytorch-cuda=12.4
    - pytorch==2.5.1
    - torchaudio==2.5.1
    - torchvision==0.20.1


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    blas-1.0                   |              mkl           6 KB
    brotlicffi-1.0.9.2         |  py310h6a678d5_1         364 KB
    certifi-2025.10.5          |  py310h06a4308_0         157 KB
    cffi-1.15.1                |  py310h74dc2b5_0         407 KB
    cuda-cudart-12.4.127       |                0         198 KB  nvidia
    cuda-cupti-12.4.127        |                0        16.4 MB  nvidia
    cuda-libraries-12.4.1      |                0           2 KB  nvidia
    cuda-nvrtc-12.4.127        |                0        21.0 MB  nvidia
    cuda-nvtx-12.4.127         |                0          58 KB  nvidia
    cuda-opencl-13.0.85        |       hf384b4a_0          26 KB  nvidia
    cuda-runtime-12.4.1        |                0           2 KB  nvidia
    cuda-version-13.0          |                3          17 KB  nvidia
    ffmpeg-4.2.2               |       h20bf706_0        59.6 MB
    filelock-3.17.0            |  py310h06a4308_0          32 KB
    freetype-2.13.3            |       h4a9f257_0         686 KB
    giflib-5.2.2               |       h5eee18b_0          80 KB
    gmp-6.3.0                  |       h6a678d5_0         608 KB
    gmpy2-2.2.1                |  py310h5eee18b_0         248 KB
    gnutls-3.6.15              |       he1e5248_0         1.0 MB
    idna-3.7                   |  py310h06a4308_0         130 KB
    intel-openmp-2025.0.0      |    h06a4308_1171        22.3 MB
    jinja2-3.1.6               |  py310h06a4308_0         277 KB
    jpeg-9f                    |       h5ce9db8_0         242 KB
    lame-3.100                 |       h7b6447c_0         323 KB
    lcms2-2.16                 |       h92b89f2_1         269 KB
    lerc-4.0.0                 |       h6a678d5_0         261 KB
    libcublas-12.4.5.8         |                0       309.2 MB  nvidia
    libcufft-11.2.1.3          |                0       190.5 MB  nvidia
    libcufile-1.15.1.6         |       h2e68323_0         957 KB  nvidia
    libcurand-10.4.0.35        |       h50d9014_0        41.5 MB  nvidia
    libcusolver-11.6.1.9       |                0       114.0 MB  nvidia
    libcusparse-12.3.1.170     |                0       179.6 MB  nvidia
    libdeflate-1.22            |       h5eee18b_0          68 KB
    libidn2-2.3.4              |       h5eee18b_0         146 KB
    libjpeg-turbo-2.0.0        |       h9bf148f_0         950 KB  pytorch
    libnpp-12.2.5.30           |                0       142.8 MB  nvidia
    libnvfatbin-13.0.85        |       hf384b4a_0         807 KB  nvidia
    libnvjitlink-12.4.127      |                0        18.2 MB  nvidia
    libnvjpeg-12.3.1.117       |                0         3.0 MB  nvidia
    libopus-1.3.1              |       h5eee18b_1         345 KB
    libpng-1.6.39              |       h5eee18b_0         304 KB
    libtasn1-4.20.0            |       ha30cf84_0          70 KB
    libtiff-4.7.0              |       hde9077f_0         447 KB
    libunistring-0.9.10        |       h27cfd23_0         536 KB
    libvpx-1.7.0               |       h439df22_0         1.2 MB
    libwebp-1.3.2              |       h9f374a3_1          94 KB
    libwebp-base-1.3.2         |       h5eee18b_1         425 KB
    llvm-openmp-14.0.6         |       h9e868ea_0         4.4 MB
    markupsafe-3.0.2           |  py310h5eee18b_0          24 KB
    mkl-2025.0.0               |     hacee8c2_941       127.4 MB
    mkl-service-2.5.2          |  py310hacdc0fc_0          72 KB
    mkl_fft-1.3.11             |  py310hacdc0fc_1         219 KB
    mkl_random-1.2.8           |  py310h2fd27a0_1         321 KB
    mpc-1.3.1                  |       h5eee18b_0         129 KB
    mpfr-4.2.1                 |       h5eee18b_0         821 KB
    mpmath-1.3.0               |  py310h06a4308_0         834 KB
    nettle-3.7.3               |       hbbd107a_1         809 KB
    networkx-3.4.2             |  py310h06a4308_0         2.5 MB
    numpy-1.26.4               |  py310h64c44e4_1          13 KB
    numpy-base-1.26.4          |  py310he1678cf_1         7.1 MB
    ocl-icd-2.3.2              |       h5eee18b_1         136 KB
    openh264-2.1.1             |       h4ff587b_0         711 KB
    openjpeg-2.5.2             |       h0d4d230_1         373 KB
    pillow-11.1.0              |  py310hac6e08b_1         818 KB
    pycparser-2.23             |  py310h06a4308_0         227 KB
    pysocks-1.7.1              |  py310h06a4308_0          28 KB
    pytorch-2.5.1              |py3.10_cuda12.4_cudnn9.1.0_0        1.46 GB  pytorch
    pytorch-cuda-12.4          |       hc786d27_7           7 KB  pytorch
    pytorch-mutex-1.0          |             cuda           3 KB  pytorch
    pyyaml-6.0.2               |  py310h5eee18b_0         193 KB
    requests-2.32.5            |  py310h06a4308_0         150 KB
    sympy-1.14.0               |  py310h06a4308_0        11.5 MB
    tbb-2022.0.0               |       hdb19cb5_0         188 KB
    tbb-devel-2022.0.0         |       hdb19cb5_0         1.1 MB
    torchaudio-2.5.1           |      py310_cu124         6.2 MB  pytorch
    torchtriton-3.1.0          |            py310       233.5 MB  pytorch
    torchvision-0.20.1         |      py310_cu124         8.2 MB  pytorch
    typing_extensions-4.15.0   |  py310h06a4308_0          80 KB
    urllib3-2.5.0              |  py310h06a4308_0         308 KB
    x264-1!157.20191217        |       h7b6447c_0         922 KB
    yaml-0.2.5                 |       h7b6447c_0          75 KB
    zstd-1.5.7                 |       h11fc155_0         608 KB
    ------------------------------------------------------------
                                           Total:        2.96 GB

The following NEW packages will be INSTALLED:

  blas               pkgs/main/linux-64::blas-1.0-mkl
  brotlicffi         pkgs/main/linux-64::brotlicffi-1.0.9.2-py310h6a678d5_1
  certifi            pkgs/main/linux-64::certifi-2025.10.5-py310h06a4308_0
  cffi               pkgs/main/linux-64::cffi-1.15.1-py310h74dc2b5_0
  charset-normalizer pkgs/main/noarch::charset-normalizer-3.3.2-pyhd3eb1b0_0
  cuda-cudart        nvidia/linux-64::cuda-cudart-12.4.127-0
  cuda-cupti         nvidia/linux-64::cuda-cupti-12.4.127-0
  cuda-libraries     nvidia/linux-64::cuda-libraries-12.4.1-0
  cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-12.4.127-0
  cuda-nvtx          nvidia/linux-64::cuda-nvtx-12.4.127-0
  cuda-opencl        nvidia/linux-64::cuda-opencl-13.0.85-hf384b4a_0
  cuda-runtime       nvidia/linux-64::cuda-runtime-12.4.1-0
  cuda-version       nvidia/noarch::cuda-version-13.0-3
  ffmpeg             pkgs/main/linux-64::ffmpeg-4.2.2-h20bf706_0
  filelock           pkgs/main/linux-64::filelock-3.17.0-py310h06a4308_0
  freetype           pkgs/main/linux-64::freetype-2.13.3-h4a9f257_0
  giflib             pkgs/main/linux-64::giflib-5.2.2-h5eee18b_0
  gmp                pkgs/main/linux-64::gmp-6.3.0-h6a678d5_0
  gmpy2              pkgs/main/linux-64::gmpy2-2.2.1-py310h5eee18b_0
  gnutls             pkgs/main/linux-64::gnutls-3.6.15-he1e5248_0
  idna               pkgs/main/linux-64::idna-3.7-py310h06a4308_0
  intel-openmp       pkgs/main/linux-64::intel-openmp-2025.0.0-h06a4308_1171
  jinja2             pkgs/main/linux-64::jinja2-3.1.6-py310h06a4308_0
  jpeg               pkgs/main/linux-64::jpeg-9f-h5ce9db8_0
  lame               pkgs/main/linux-64::lame-3.100-h7b6447c_0
  lcms2              pkgs/main/linux-64::lcms2-2.16-h92b89f2_1
  lerc               pkgs/main/linux-64::lerc-4.0.0-h6a678d5_0
  libcublas          nvidia/linux-64::libcublas-12.4.5.8-0
  libcufft           nvidia/linux-64::libcufft-11.2.1.3-0
  libcufile          nvidia/linux-64::libcufile-1.15.1.6-h2e68323_0
  libcurand          nvidia/linux-64::libcurand-10.4.0.35-h50d9014_0
  libcusolver        nvidia/linux-64::libcusolver-11.6.1.9-0
  libcusparse        nvidia/linux-64::libcusparse-12.3.1.170-0
  libdeflate         pkgs/main/linux-64::libdeflate-1.22-h5eee18b_0
  libidn2            pkgs/main/linux-64::libidn2-2.3.4-h5eee18b_0
  libjpeg-turbo      pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  libnpp             nvidia/linux-64::libnpp-12.2.5.30-0
  libnvfatbin        nvidia/linux-64::libnvfatbin-13.0.85-hf384b4a_0
  libnvjitlink       nvidia/linux-64::libnvjitlink-12.4.127-0
  libnvjpeg          nvidia/linux-64::libnvjpeg-12.3.1.117-0
  libopus            pkgs/main/linux-64::libopus-1.3.1-h5eee18b_1
  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0
  libtasn1           pkgs/main/linux-64::libtasn1-4.20.0-ha30cf84_0
  libtiff            pkgs/main/linux-64::libtiff-4.7.0-hde9077f_0
  libunistring       pkgs/main/linux-64::libunistring-0.9.10-h27cfd23_0
  libvpx             pkgs/main/linux-64::libvpx-1.7.0-h439df22_0
  libwebp            pkgs/main/linux-64::libwebp-1.3.2-h9f374a3_1
  libwebp-base       pkgs/main/linux-64::libwebp-base-1.3.2-h5eee18b_1
  llvm-openmp        pkgs/main/linux-64::llvm-openmp-14.0.6-h9e868ea_0
  lz4-c              pkgs/main/linux-64::lz4-c-1.9.4-h6a678d5_1
  markupsafe         pkgs/main/linux-64::markupsafe-3.0.2-py310h5eee18b_0
  mkl                pkgs/main/linux-64::mkl-2025.0.0-hacee8c2_941
  mkl-service        pkgs/main/linux-64::mkl-service-2.5.2-py310hacdc0fc_0
  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.11-py310hacdc0fc_1
  mkl_random         pkgs/main/linux-64::mkl_random-1.2.8-py310h2fd27a0_1
  mpc                pkgs/main/linux-64::mpc-1.3.1-h5eee18b_0
  mpfr               pkgs/main/linux-64::mpfr-4.2.1-h5eee18b_0
  mpmath             pkgs/main/linux-64::mpmath-1.3.0-py310h06a4308_0
  nettle             pkgs/main/linux-64::nettle-3.7.3-hbbd107a_1
  networkx           pkgs/main/linux-64::networkx-3.4.2-py310h06a4308_0
  numpy              pkgs/main/linux-64::numpy-1.26.4-py310h64c44e4_1
  numpy-base         pkgs/main/linux-64::numpy-base-1.26.4-py310he1678cf_1
  ocl-icd            pkgs/main/linux-64::ocl-icd-2.3.2-h5eee18b_1
  openh264           pkgs/main/linux-64::openh264-2.1.1-h4ff587b_0
  openjpeg           pkgs/main/linux-64::openjpeg-2.5.2-h0d4d230_1
  pillow             pkgs/main/linux-64::pillow-11.1.0-py310hac6e08b_1
  pycparser          pkgs/main/linux-64::pycparser-2.23-py310h06a4308_0
  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py310h06a4308_0
  pytorch            pytorch/linux-64::pytorch-2.5.1-py3.10_cuda12.4_cudnn9.1.0_0
  pytorch-cuda       pytorch/linux-64::pytorch-cuda-12.4-hc786d27_7
  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda
  pyyaml             pkgs/main/linux-64::pyyaml-6.0.2-py310h5eee18b_0
  requests           pkgs/main/linux-64::requests-2.32.5-py310h06a4308_0
  sympy              pkgs/main/linux-64::sympy-1.14.0-py310h06a4308_0
  tbb                pkgs/main/linux-64::tbb-2022.0.0-hdb19cb5_0
  tbb-devel          pkgs/main/linux-64::tbb-devel-2022.0.0-hdb19cb5_0
  torchaudio         pytorch/linux-64::torchaudio-2.5.1-py310_cu124
  torchtriton        pytorch/linux-64::torchtriton-3.1.0-py310
  torchvision        pytorch/linux-64::torchvision-0.20.1-py310_cu124
  typing_extensions  pkgs/main/linux-64::typing_extensions-4.15.0-py310h06a4308_0
  urllib3            pkgs/main/linux-64::urllib3-2.5.0-py310h06a4308_0
  x264               pkgs/main/linux-64::x264-1!157.20191217-h7b6447c_0
  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0
  zstd               pkgs/main/linux-64::zstd-1.5.7-h11fc155_0


Proceed ([y]/n)?


Downloading and Extracting Packages:

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ python -m pip install -r requirements.txt
Collecting accelerate==1.9.0 (from -r requirements.txt (line 1))
  Downloading accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)
Collecting av==15.0.0 (from -r requirements.txt (line 2))
  Downloading av-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.6 kB)
Collecting certifi==2025.8.3 (from -r requirements.txt (line 3))
  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)
Collecting charset-normalizer==3.4.2 (from -r requirements.txt (line 4))
  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
ERROR: Ignored the following versions that require a different python version: 1.3.3 Requires-Python >=3.11
ERROR: Could not find a version that satisfies the requirement contourpy==1.3.3 (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.1.0, 1.1.1rc1, 1.1.1, 1.2.0, 1.2.1rc1, 1.2.1, 1.3.0, 1.3.1, 1.3.2)
ERROR: No matching distribution found for contourpy==1.3.3
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ vim requirements.txt
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ python -m pip install -r requirements.txt
Collecting accelerate==1.9.0 (from -r requirements.txt (line 1))
  Using cached accelerate-1.9.0-py3-none-any.whl.metadata (19 kB)
Collecting av==15.0.0 (from -r requirements.txt (line 2))
  Using cached av-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (4.6 kB)
Collecting certifi==2025.8.3 (from -r requirements.txt (line 3))
  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)
Collecting charset-normalizer==3.4.2 (from -r requirements.txt (line 4))
  Using cached charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
Collecting contourpy==1.3.2 (from -r requirements.txt (line 5))
  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)
Collecting cycler==0.12.1 (from -r requirements.txt (line 6))
  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)
Collecting decord==0.6.0 (from -r requirements.txt (line 7))
  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)
Collecting diffusers==0.34.0 (from -r requirements.txt (line 8))
  Downloading diffusers-0.34.0-py3-none-any.whl.metadata (20 kB)
Collecting einops==0.8.1 (from -r requirements.txt (line 9))
  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)
Collecting filelock==3.13.1 (from -r requirements.txt (line 10))
  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)
Collecting fonttools==4.59.0 (from -r requirements.txt (line 11))
  Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)
Collecting fsspec==2024.6.1 (from -r requirements.txt (line 12))
  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)
Collecting hf-xet==1.1.5 (from -r requirements.txt (line 13))
  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)
Collecting huggingface-hub==0.34.3 (from -r requirements.txt (line 14))
  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)
Collecting idna==3.10 (from -r requirements.txt (line 15))
  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting imageio==2.37.0 (from -r requirements.txt (line 16))
  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)
Collecting imageio-ffmpeg==0.6.0 (from -r requirements.txt (line 17))
  Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting importlib_metadata==8.7.0 (from -r requirements.txt (line 18))
  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)
Collecting Jinja2==3.1.4 (from -r requirements.txt (line 19))
  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)
Collecting kiwisolver==1.4.8 (from -r requirements.txt (line 20))
  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)
Collecting loguru==0.7.3 (from -r requirements.txt (line 21))
  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)
Collecting MarkupSafe==2.1.5 (from -r requirements.txt (line 22))
  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
Collecting matplotlib==3.10.5 (from -r requirements.txt (line 23))
  Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)
Requirement already satisfied: mpmath==1.3.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (1.3.0)
Collecting networkx==3.3 (from -r requirements.txt (line 25))
  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)
Collecting ninja==1.11.1.4 (from -r requirements.txt (line 26))
  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)
Collecting numpy==2.1.2 (from -r requirements.txt (line 27))
  Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)
Collecting nvidia-ml-py==12.575.51 (from -r requirements.txt (line 28))
  Downloading nvidia_ml_py-12.575.51-py3-none-any.whl.metadata (9.3 kB)
Collecting nvidia-nccl-cu12==2.21.5 (from -r requirements.txt (line 29))
  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.4.127 (from -r requirements.txt (line 30))
  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-nvtx-cu12==12.4.127 (from -r requirements.txt (line 31))
  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)
Collecting nvitop==1.5.2 (from -r requirements.txt (line 32))
  Downloading nvitop-1.5.2-py3-none-any.whl.metadata (80 kB)
Collecting opencv-python-headless==4.12.0.88 (from -r requirements.txt (line 33))
  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)
Collecting packaging==25.0 (from -r requirements.txt (line 34))
  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
Collecting pandas==2.3.1 (from -r requirements.txt (line 35))
  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)
Collecting pillow==11.0.0 (from -r requirements.txt (line 36))
  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)
Collecting protobuf==6.31.1 (from -r requirements.txt (line 37))
  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)
Collecting psutil==7.0.0 (from -r requirements.txt (line 38))
  Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)
Collecting pyparsing==3.2.3 (from -r requirements.txt (line 39))
  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)
Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 40))
  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)
Collecting pytz==2025.2 (from -r requirements.txt (line 41))
  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)
Requirement already satisfied: PyYAML==6.0.2 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (6.0.2)
Collecting regex==2025.7.34 (from -r requirements.txt (line 43))
  Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)
Collecting requests==2.32.4 (from -r requirements.txt (line 44))
  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)
Collecting safetensors==0.5.3 (from -r requirements.txt (line 45))
  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)
Collecting sentencepiece==0.2.0 (from -r requirements.txt (line 46))
  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting setuptools==78.1.1 (from -r requirements.txt (line 47))
  Downloading setuptools-78.1.1-py3-none-any.whl.metadata (6.5 kB)
Collecting six==1.17.0 (from -r requirements.txt (line 48))
  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting sympy==1.13.1 (from -r requirements.txt (line 49))
  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)
Collecting tokenizers==0.21.4 (from -r requirements.txt (line 50))
  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting tqdm==4.67.1 (from -r requirements.txt (line 51))
  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting transformers==4.54.1 (from -r requirements.txt (line 52))
  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)
Requirement already satisfied: triton==3.1.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from -r requirements.txt (line 53)) (3.1.0)
Collecting typing_extensions==4.12.2 (from -r requirements.txt (line 54))
  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)
Collecting tzdata==2025.2 (from -r requirements.txt (line 55))
  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)
Requirement already satisfied: urllib3==2.5.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (2.5.0)
Requirement already satisfied: wheel==0.45.1 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from -r requirements.txt (line 57)) (0.45.1)
Collecting zipp==3.23.0 (from -r requirements.txt (line 58))
  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)
Requirement already satisfied: torch>=2.0.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from accelerate==1.9.0->-r requirements.txt (line 1)) (2.5.1)
Downloading accelerate-1.9.0-py3-none-any.whl (367 kB)
Downloading numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/16.3 MB 139.2 MB/s  0:00:00
Downloading av-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.2/39.2 MB 185.2 MB/s  0:00:00
Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)
Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)
Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)
Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)
Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/13.6 MB 146.4 MB/s  0:00:00
Downloading diffusers-0.34.0-py3-none-any.whl (3.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 134.1 MB/s  0:00:00
Downloading einops-0.8.1-py3-none-any.whl (64 kB)
Downloading filelock-3.13.1-py3-none-any.whl (11 kB)
Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 158.4 MB/s  0:00:00
Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)
Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 134.2 MB/s  0:00:00
Downloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 558.8/558.8 kB 34.4 MB/s  0:00:00
Downloading idna-3.10-py3-none-any.whl (70 kB)
Downloading imageio-2.37.0-py3-none-any.whl (315 kB)
Downloading imageio_ffmpeg-0.6.0-py3-none-manylinux2014_x86_64.whl (29.5 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.5/29.5 MB 149.1 MB/s  0:00:00
Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)
Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)
Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 8.9 MB/s  0:00:00
Downloading loguru-0.7.3-py3-none-any.whl (61 kB)
Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)
Downloading matplotlib-3.10.5-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.7/8.7 MB 30.6 MB/s  0:00:00
Downloading networkx-3.3-py3-none-any.whl (1.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 9.4 MB/s  0:00:00
Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)
Downloading nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)
Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 48.2 MB/s  0:00:03
Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 42.4 MB/s  0:00:00
Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)
Downloading nvitop-1.5.2-py3-none-any.whl (213 kB)
Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 MB 45.8 MB/s  0:00:01
Downloading packaging-25.0-py3-none-any.whl (66 kB)
Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 32.6 MB/s  0:00:00
Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 25.4 MB/s  0:00:00
Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)
Downloading psutil-7.0.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (277 kB)
Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)
Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)
Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)
Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 789.8/789.8 kB 43.6 MB/s  0:00:00
Downloading requests-2.32.4-py3-none-any.whl (64 kB)
Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)
Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 71.7 MB/s  0:00:00
Downloading setuptools-78.1.1-py3-none-any.whl (1.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 69.1 MB/s  0:00:00
Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)
Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 129.8 MB/s  0:00:00
Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 144.7 MB/s  0:00:00
Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)
Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.2/11.2 MB 178.9 MB/s  0:00:00
Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)
Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)
Downloading zipp-3.23.0-py3-none-any.whl (10 kB)
Installing collected packages: sentencepiece, pytz, nvidia-ml-py, zipp, tzdata, typing_extensions, tqdm, sympy, six, setuptools, safetensors, regex, pyparsing, psutil, protobuf, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, numpy, ninja, networkx, M
arkupSafe, loguru, kiwisolver, imageio-ffmpeg, idna, hf-xet, fsspec, fonttools, filelock, einops, cycler, charset-normalizer, certifi, av, requests, python-dateutil, opencv-python-headless, nvitop, Jinja2, importlib_metadata, imageio, decord, contourpy, pandas, matplotlib, hugging
face-hub, tokenizers, diffusers, accelerate, transformers
  Attempting uninstall: typing_extensions
    Found existing installation: typing_extensions 4.15.0
    Uninstalling typing_extensions-4.15.0:
      Successfully uninstalled typing_extensions-4.15.0
  Attempting uninstall: sympy
    Found existing installation: sympy 1.14.0
    Uninstalling sympy-1.14.0:
      Successfully uninstalled sympy-1.14.0
  Attempting uninstall: setuptools
    Found existing installation: setuptools 80.9.0
    Uninstalling setuptools-80.9.0:
      Successfully uninstalled setuptools-80.9.0
  Attempting uninstall: pillow
    Found existing installation: pillow 11.1.0
    Uninstalling pillow-11.1.0:
      Successfully uninstalled pillow-11.1.0
  Attempting uninstall: numpy
    Found existing installation: numpy 1.26.4
    Uninstalling numpy-1.26.4:
      Successfully uninstalled numpy-1.26.4
  Attempting uninstall: networkx
    Found existing installation: networkx 3.4.2
    Uninstalling networkx-3.4.2:
      Successfully uninstalled networkx-3.4.2
  Attempting uninstall: MarkupSafe
    Found existing installation: MarkupSafe 3.0.2
    Uninstalling MarkupSafe-3.0.2:
      Successfully uninstalled MarkupSafe-3.0.2
  Attempting uninstall: idna
    Found existing installation: idna 3.7
    Uninstalling idna-3.7:
      Successfully uninstalled idna-3.7
  Attempting uninstall: filelock
    Found existing installation: filelock 3.17.0
    Uninstalling filelock-3.17.0:
      Successfully uninstalled filelock-3.17.0
  Attempting uninstall: charset-normalizer
    Found existing installation: charset-normalizer 3.3.2
    Uninstalling charset-normalizer-3.3.2:
      Successfully uninstalled charset-normalizer-3.3.2
  Attempting uninstall: certifi
    Found existing installation: certifi 2025.10.5
    Uninstalling certifi-2025.10.5:
      Successfully uninstalled certifi-2025.10.5
  Attempting uninstall: requests
    Found existing installation: requests 2.32.5
    Uninstalling requests-2.32.5:
      Successfully uninstalled requests-2.32.5
  Attempting uninstall: Jinja2
    Found existing installation: Jinja2 3.1.6
    Uninstalling Jinja2-3.1.6:
      Successfully uninstalled Jinja2-3.1.6
Successfully installed Jinja2-3.1.4 MarkupSafe-2.1.5 accelerate-1.9.0 av-15.0.0 certifi-2025.8.3 charset-normalizer-3.4.2 contourpy-1.3.2 cycler-0.12.1 decord-0.6.0 diffusers-0.34.0 einops-0.8.1 filelock-3.13.1 fonttools-4.59.0 fsspec-2024.6.1 hf-xet-1.1.5 huggingface-hub-0.34.3 i
dna-3.10 imageio-2.37.0 imageio-ffmpeg-0.6.0 importlib_metadata-8.7.0 kiwisolver-1.4.8 loguru-0.7.3 matplotlib-3.10.5 networkx-3.3 ninja-1.11.1.4 numpy-2.1.2 nvidia-ml-py-12.575.51 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 nvitop-1.5.2 opencv
-python-headless-4.12.0.88 packaging-25.0 pandas-2.3.1 pillow-11.0.0 protobuf-6.31.1 psutil-7.0.0 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 regex-2025.7.34 requests-2.32.4 safetensors-0.5.3 sentencepiece-0.2.0 setuptools-78.1.1 six-1.17.0 sympy-1.13.1 tokenizers-0.21
.4 tqdm-4.67.1 transformers-4.54.1 typing_extensions-4.12.2 tzdata-2025.2 zipp-3.23.0
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ python -m pip install ninja
Requirement already satisfied: ninja in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (1.11.1.4)
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip install flash-attn==2.6.3 --no-build-isolation
Collecting flash-attn==2.6.3
  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.6/2.6 MB 9.4 MB/s  0:00:00
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [8 lines of output]
      Traceback (most recent call last):
        File "<string>", line 2, in <module>
        File "<pip-setuptools-caller>", line 35, in <module>
        File "/tmp/pip-install-_z326f4d/flash-attn_e81cba0abe484f94bd22ded2ceb0ff73/setup.py", line 21, in <module>
          import torch
        File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/__init__.py", line 367, in <module>
          from torch._C import *  # noqa: F403
      ImportError: /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
Traceback (most recent call last):
  File "<string>", line 1, in <module>
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/__init__.py", line 367, in <module>
    from torch._C import *  # noqa: F403
ImportError: /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ nvidia-smi --query-gpu=name,compute_cap --format=csv
name, compute_cap
NVIDIA L40S, 8.9
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.5.1', 'console_scripts', 'torchrun')())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 25, in importlib_load_entry_point
    return next(matches).load()
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/importlib/metadata/__init__.py", line 162, in load
    module = import_module(match.group('module'))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 992, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/__init__.py", line 367, in <module>
    from torch._C import *  # noqa: F403
ImportError: /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip uninstall torch torchvision torchaudio -y
Found existing installation: torch 2.5.1
Uninstalling torch-2.5.1:
  Successfully uninstalled torch-2.5.1
Found existing installation: torchvision 0.20.1
Uninstalling torchvision-0.20.1:
  Successfully uninstalled torchvision-0.20.1
Found existing installation: torchaudio 2.5.1
Uninstalling torchaudio-2.5.1:
  Successfully uninstalled torchaudio-2.5.1
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
Looking in indexes: https://download.pytorch.org/whl/cu124
Collecting torch
  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (28 kB)
Collecting torchvision
  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (6.1 kB)
Collecting torchaudio
  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl.metadata (6.6 kB)
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (3.13.1)
Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (3.3)
Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (3.1.4)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (2024.6.1)
Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 193.5 MB/s  0:00:00
Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 67.4 MB/s  0:00:00
Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 206.8 MB/s  0:00:00
Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 48.7 MB/s  0:00:06
Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 95.8 MB/s  0:00:03
Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 167.8 MB/s  0:00:01
Collecting nvidia-curand-cu12==10.3.5.147 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 224.8 MB/s  0:00:00
Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 176.7 MB/s  0:00:00
Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 141.3 MB/s  0:00:01
Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)
  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (12.4.127)
Collecting triton==3.2.0 (from torch)
  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)
Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torchvision) (2.1.2)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torchvision) (11.0.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)
Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (768.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 768.4/768.4 MB 42.2 MB/s  0:00:07
Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.1/150.1 MB 151.8 MB/s  0:00:00
Downloading https://download.pytorch.org/whl/triton-3.2.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.6/166.6 MB 131.9 MB/s  0:00:01
Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp310-cp310-linux_x86_64.whl (7.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/7.3 MB 188.1 MB/s  0:00:00
Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp310-cp310-linux_x86_64.whl (3.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 154.0 MB/s  0:00:00
Installing collected packages: triton, nvidia-cusparselt-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, to
rchaudio
  Attempting uninstall: triton
    Found existing installation: triton 3.1.0
    Uninstalling triton-3.1.0:
      Successfully uninstalled triton-3.1.0
Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu
12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 torch-2.6.0+cu124 torchaudio-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"

2.6.0+cu124
True
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip install flash-attn==2.6.3 --no-build-isolation
Collecting flash-attn==2.6.3
  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: torch in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from flash-attn==2.6.3) (2.6.0+cu124)
Requirement already satisfied: einops in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from flash-attn==2.6.3) (0.8.1)
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.13.1)
Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (4.12.2)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.3)
Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.1.4)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (2024.6.1)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.4.127)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.4.127)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.4.127)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.4.5.8)
Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (11.2.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (10.3.5.147)
Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (11.6.1.9)
Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.3.1.170)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (0.6.2)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.4.127)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.4.127)
Requirement already satisfied: triton==3.2.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.2.0)
Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (1.13.1)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn==2.6.3) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.6.3) (2.1.5)
Building wheels for collected packages: flash-attn
  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (po
ssibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for flash-attn (setup.py) ... /^canceled
ERROR: Operation cancelled by user
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip uninstall torch torchvision torchaudio -y
Found existing installation: torch 2.6.0+cu124
Uninstalling torch-2.6.0+cu124:
  Successfully uninstalled torch-2.6.0+cu124
Found existing installation: torchvision 0.21.0+cu124
Uninstalling torchvision-0.21.0+cu124:
  Successfully uninstalled torchvision-0.21.0+cu124
Found existing installation: torchaudio 2.6.0+cu124
Uninstalling torchaudio-2.6.0+cu124:
  Successfully uninstalled torchaudio-2.6.0+cu124
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
Looking in indexes: https://download.pytorch.org/whl/cu121
Collecting torch
  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (780.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 780.4/780.4 MB 39.4 MB/s  0:00:08
Collecting torchvision
  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.3/7.3 MB 168.3 MB/s  0:00:00
Collecting torchaudio
  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 140.6 MB/s  0:00:00
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (3.13.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (4.12.2)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (3.3)
Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (3.1.4)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (2024.6.1)
Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 202.3 MB/s  0:00:00
Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 70.9 MB/s  0:00:00
Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 184.9 MB/s  0:00:00
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (9.1.0.70)
Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 79.9 MB/s  0:00:03
Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 205.8 MB/s  0:00:00
Collecting nvidia-curand-cu12==10.3.2.106 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 186.9 MB/s  0:00:00
Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 191.5 MB/s  0:00:00
Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 147.8 MB/s  0:00:01
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (2.21.5)
Collecting nvidia-nvtx-cu12==12.1.105 (from torch)
  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)
Collecting triton==3.1.0 (from torch)
  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.5/209.5 MB 120.7 MB/s  0:00:01
Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch) (1.13.1)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)
Requirement already satisfied: numpy in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torchvision) (2.1.2)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torchvision) (11.0.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)
Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio
  Attempting uninstall: triton
    Found existing installation: triton 3.2.0
    Uninstalling triton-3.2.0:
      Successfully uninstalled triton-3.2.0
  Attempting uninstall: nvidia-nvtx-cu12
    Found existing installation: nvidia-nvtx-cu12 12.4.127
    Uninstalling nvidia-nvtx-cu12-12.4.127:
      Successfully uninstalled nvidia-nvtx-cu12-12.4.127
  Attempting uninstall: nvidia-cusparse-cu12
    Found existing installation: nvidia-cusparse-cu12 12.3.1.170
    Uninstalling nvidia-cusparse-cu12-12.3.1.170:
      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170
  Attempting uninstall: nvidia-curand-cu12
    Found existing installation: nvidia-curand-cu12 10.3.5.147
    Uninstalling nvidia-curand-cu12-10.3.5.147:
      Successfully uninstalled nvidia-curand-cu12-10.3.5.147
  Attempting uninstall: nvidia-cufft-cu12
    Found existing installation: nvidia-cufft-cu12 11.2.1.3
    Uninstalling nvidia-cufft-cu12-11.2.1.3:
      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3
  Attempting uninstall: nvidia-cuda-runtime-cu12
    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127
    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:
      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127
  Attempting uninstall: nvidia-cuda-nvrtc-cu12
    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127
    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:
      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127
  Attempting uninstall: nvidia-cuda-cupti-cu12
    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127
    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:
      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127
  Attempting uninstall: nvidia-cublas-cu12
    Found existing installation: nvidia-cublas-cu12 12.4.5.8
    Uninstalling nvidia-cublas-cu12-12.4.5.8:
      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8
  Attempting uninstall: nvidia-cusolver-cu12
    Found existing installation: nvidia-cusolver-cu12 11.6.1.9
    Uninstalling nvidia-cusolver-cu12-11.6.1.9:
      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9
Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nvt
x-cu12-12.1.105 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip install flash-attn==2.6.3 --index-url https://download.pytorch.org/whl/cu121
Looking in indexes: https://download.pytorch.org/whl/cu121
ERROR: Could not find a version that satisfies the requirement flash-attn==2.6.3 (from versions: none)
ERROR: No matching distribution found for flash-attn==2.6.3
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ pip install flash-attn==2.6.3 --no-build-isolation
Collecting flash-attn==2.6.3
  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)
  Preparing metadata (setup.py) ... done
Requirement already satisfied: torch in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from flash-attn==2.6.3) (2.5.1+cu121)
Requirement already satisfied: einops in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from flash-attn==2.6.3) (0.8.1)
Requirement already satisfied: filelock in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.13.1)
Requirement already satisfied: typing-extensions>=4.8.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (4.12.2)
Requirement already satisfied: networkx in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.3)
Requirement already satisfied: jinja2 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.1.4)
Requirement already satisfied: fsspec in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (2024.6.1)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.1.105)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (9.1.0.70)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.1.3.1)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (11.0.2.54)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (10.3.2.106)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (11.4.5.107)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.1.0.106)
Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (2.21.5)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (12.1.105)
Requirement already satisfied: triton==3.1.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (3.1.0)
Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from torch->flash-attn==2.6.3) (1.13.1)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.6.3) (12.4.127)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn==2.6.3) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages (from jinja2->torch->flash-attn==2.6.3) (2.1.5)
Building wheels for collected packages: flash-attn
  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (po
ssibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334
  Building wheel for flash-attn (setup.py) ... |^canceled
ERROR: Operation cancelled by user
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
Traceback (most recent call last):
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 15, in <module>
    from hymm_sp.sample_inference import HunyuanVideoSampler
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py", line 12, in <module>
    from hymm_sp.diffusion import load_diffusion_pipeline
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/diffusion/__init__.py", line 1, in <module>
    from .pipelines import HunyuanVideoGamePipeline
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/diffusion/pipelines/__init__.py", line 5, in <module>
    from .pipeline_hunyuan_video_game import HunyuanVideoGamePipeline
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/diffusion/pipelines/pipeline_hunyuan_video_game.py", line 45, in <module>
    from hymm_sp.vae.autoencoder_kl_causal_3d import AutoencoderKLCausal3D
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/vae/__init__.py", line 3, in <module>
    from .autoencoder_kl_causal_3d import AutoencoderKLCausal3D
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/vae/autoencoder_kl_causal_3d.py", line 34, in <module>
    from hymm_sp.modules.parallel_states import (
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/__init__.py", line 1, in <module>
    from .models import HYVideoDiffusionTransformer, HUNYUAN_VIDEO_CONFIG
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 9, in <module>
    from flash_attn.flash_attn_interface import flash_attn_varlen_func
ModuleNotFoundError: No module named 'flash_attn'
E1015 20:49:51.879831 35071 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 35101) of binary: /home/ubuntu/miniconda3/envs/HYGameCraft/bin/python3.10
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
hymm_sp/sample_batch.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-15_20:49:51
  host      : ip-172-31-1-60.us-west-2.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 35101)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
models: cpu_offload=1, DISABLE_SP=1
TrtRunner or EngineFromBytes is not available, you can not use trt engine
text_encoder: cpu_offload=1
2025-10-15 20:50:37.098 | INFO     | __main__:main:85 - ********************
2025-10-15 20:50:37.196 | INFO     | __main__:main:91 - ++++++++++++++++++++
2025-10-15 20:50:37.197 | INFO     | __main__:main:96 - Generated videos will be saved to: ./results_poor/
2025-10-15 20:50:37.197 | INFO     | __main__:main:107 - Loading model from checkpoint: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 20:50:37.197 | INFO     | hymm_sp.inference:from_pretrained:62 - Got text-to-video model root path: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 20:50:37.197 | INFO     | hymm_sp.inference:from_pretrained:70 - Building model...
========================= build model =========================
==================== load transformer to cpu
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/inference.py:167: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute ar
bitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling.
 Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loade
d file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(ckpt_path, map_location=lambda storage, loc: storage)
Quantizing weights:   0%|                                                                                                                                                                                                                                       | 0/1142 [00:00<?, ?it/s]
Warning: txt_in.input_embedder is not in a block module, skipping
Warning: txt_in.t_embedder.mlp.0 is not in a block module, skipping
Warning: txt_in.t_embedder.mlp.2 is not in a block module, skipping
Warning: txt_in.c_embedder.linear_1 is not in a block module, skipping
Warning: txt_in.c_embedder.linear_2 is not in a block module, skipping
Quantizing weights:   2%|█████                                                                                                                                                                                                                        | 26/1142 [00:00<00:06, 178.26it/s]
Warning: txt_in.individual_token_refiner.blocks.0.adaLN_modulation.1 is a mod module, skipping
Quantizing weights:   4%|████████▌                                                                                                                                                                                                                    | 44/1142 [00:00<00:07, 142.13it/s]
Warning: txt_in.individual_token_refiner.blocks.1.adaLN_modulation.1 is a mod module, skipping
Warning: time_in.mlp.0 is not in a block module, skipping
Warning: time_in.mlp.2 is not in a block module, skipping
Warning: vector_in.in_layer is not in a block module, skipping
Warning: vector_in.out_layer is not in a block module, skipping
Warning: double_blocks.0.img_mod.linear is a mod module, skipping
Quantizing weights:   6%|█████████████▌                                                                                                                                                                                                               | 70/1142 [00:00<00:05, 183.05it/s]
Warning: double_blocks.0.txt_mod.linear is a mod module, skipping
Quantizing weights:   8%|█████████████████▍                                                                                                                                                                                                           | 90/1142 [00:00<00:07, 139.12it/s]
Warning: double_blocks.1.img_mod.linear is a mod module, skipping
Quantizing weights:   9%|████████████████████▍                                                                                                                                                                                                       | 106/1142 [00:00<00:08, 127.98it/s]
Warning: double_blocks.1.txt_mod.linear is a mod module, skipping
Quantizing weights:  11%|███████████████████████▌                                                                                                                                                                                                    | 122/1142 [00:00<00:08, 122.21it/s]
Warning: double_blocks.2.img_mod.linear is a mod module, skipping
Quantizing weights:  12%|██████████████████████████▊                                                                                                                                                                                                 | 139/1142 [00:01<00:08, 120.95it/s]
Warning: double_blocks.2.txt_mod.linear is a mod module, skipping
Quantizing weights:  14%|█████████████████████████████▊                                                                                                                                                                                              | 155/1142 [00:01<00:08, 117.27it/s]
Warning: double_blocks.3.img_mod.linear is a mod module, skipping
Quantizing weights:  15%|████████████████████████████████▌                                                                                                                                                                                           | 169/1142 [00:01<00:07, 122.32it/s]
Warning: double_blocks.3.txt_mod.linear is a mod module, skipping
Quantizing weights:  17%|█████████████████████████████████████▎                                                                                                                                                                                      | 194/1142 [00:01<00:08, 112.33it/s]
Warning: double_blocks.4.img_mod.linear is a mod module, skipping
Quantizing weights:  18%|████████████████████████████████████████▎                                                                                                                                                                                   | 209/1142 [00:01<00:08, 108.06it/s]
Warning: double_blocks.4.txt_mod.linear is a mod module, skipping
Quantizing weights:  20%|███████████████████████████████████████████▎                                                                                                                                                                                | 225/1142 [00:01<00:08, 109.70it/s]
Warning: double_blocks.5.img_mod.linear is a mod module, skipping
Quantizing weights:  21%|██████████████████████████████████████████████▌                                                                                                                                                                             | 242/1142 [00:01<00:07, 114.19it/s]
Warning: double_blocks.5.txt_mod.linear is a mod module, skipping
Quantizing weights:  23%|█████████████████████████████████████████████████▋                                                                                                                                                                          | 258/1142 [00:02<00:07, 112.16it/s]
Warning: double_blocks.6.img_mod.linear is a mod module, skipping
Quantizing weights:  24%|████████████████████████████████████████████████████▉                                                                                                                                                                       | 275/1142 [00:02<00:07, 115.83it/s]
Warning: double_blocks.6.txt_mod.linear is a mod module, skipping
Quantizing weights:  25%|███████████████████████████████████████████████████████▎                                                                                                                                                                    | 287/1142 [00:02<00:07, 114.24it/s]
Warning: double_blocks.7.img_mod.linear is a mod module, skipping
Quantizing weights:  27%|██████████████████████████████████████████████████████████▌                                                                                                                                                                 | 304/1142 [00:02<00:07, 115.19it/s]
Warning: double_blocks.7.txt_mod.linear is a mod module, skipping
Quantizing weights:  28%|█████████████████████████████████████████████████████████████▋                                                                                                                                                              | 320/1142 [00:02<00:07, 115.43it/s]
Warning: double_blocks.8.img_mod.linear is a mod module, skipping
Quantizing weights:  29%|████████████████████████████████████████████████████████████████▎                                                                                                                                                           | 334/1142 [00:02<00:06, 121.06it/s]
Warning: double_blocks.8.txt_mod.linear is a mod module, skipping
Quantizing weights:  31%|█████████████████████████████████████████████████████████████████████▏                                                                                                                                                      | 359/1142 [00:03<00:07, 110.89it/s]
Warning: double_blocks.9.img_mod.linear is a mod module, skipping
Quantizing weights:  33%|████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 374/1142 [00:03<00:06, 110.38it/s]
Warning: double_blocks.9.txt_mod.linear is a mod module, skipping
Quantizing weights:  34%|███████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 390/1142 [00:03<00:06, 112.46it/s]
Warning: double_blocks.10.img_mod.linear is a mod module, skipping
Quantizing weights:  36%|██████████████████████████████████████████████████████████████████████████████▍                                                                                                                                             | 407/1142 [00:03<00:06, 116.02it/s]
Warning: double_blocks.10.txt_mod.linear is a mod module, skipping
Quantizing weights:  37%|█████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                          | 423/1142 [00:03<00:06, 116.17it/s]
Warning: double_blocks.11.img_mod.linear is a mod module, skipping
Quantizing weights:  39%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 440/1142 [00:03<00:05, 118.46it/s]
Warning: double_blocks.11.txt_mod.linear is a mod module, skipping
Quantizing weights:  40%|███████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                    | 456/1142 [00:03<00:05, 117.38it/s]
Warning: double_blocks.12.img_mod.linear is a mod module, skipping
Quantizing weights:  41%|███████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                 | 473/1142 [00:03<00:05, 117.24it/s]
Warning: double_blocks.12.txt_mod.linear is a mod module, skipping
Quantizing weights:  43%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                             | 489/1142 [00:04<00:05, 114.81it/s]
Warning: double_blocks.13.img_mod.linear is a mod module, skipping
Quantizing weights:  44%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                          | 506/1142 [00:04<00:05, 117.19it/s]
Warning: double_blocks.13.txt_mod.linear is a mod module, skipping
Quantizing weights:  46%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                       | 522/1142 [00:04<00:05, 115.83it/s]
Warning: double_blocks.14.img_mod.linear is a mod module, skipping
Quantizing weights:  47%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                    | 539/1142 [00:04<00:05, 118.30it/s]
Warning: double_blocks.14.txt_mod.linear is a mod module, skipping
Quantizing weights:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                 | 555/1142 [00:04<00:05, 114.37it/s]
Warning: double_blocks.15.img_mod.linear is a mod module, skipping
Quantizing weights:  50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                              | 568/1142 [00:04<00:04, 115.24it/s]
Warning: double_blocks.15.txt_mod.linear is a mod module, skipping
Quantizing weights:  51%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                            | 581/1142 [00:04<00:04, 118.17it/s]
Warning: double_blocks.16.img_mod.linear is a mod module, skipping
Quantizing weights:  53%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                       | 607/1142 [00:05<00:04, 115.38it/s]
Warning: double_blocks.16.txt_mod.linear is a mod module, skipping
Quantizing weights:  54%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                    | 621/1142 [00:05<00:04, 110.06it/s]
Warning: double_blocks.17.img_mod.linear is a mod module, skipping
Quantizing weights:  56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                 | 638/1142 [00:05<00:04, 116.04it/s]
Warning: double_blocks.17.txt_mod.linear is a mod module, skipping
Quantizing weights:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 654/1142 [00:05<00:04, 117.68it/s]
Warning: double_blocks.18.img_mod.linear is a mod module, skipping
Quantizing weights:  59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                          | 671/1142 [00:05<00:03, 121.03it/s]
Warning: double_blocks.18.txt_mod.linear is a mod module, skipping
Quantizing weights:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                       | 687/1142 [00:05<00:03, 120.32it/s]
Warning: double_blocks.19.img_mod.linear is a mod module, skipping
Quantizing weights:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 704/1142 [00:05<00:03, 123.08it/s]
Warning: double_blocks.19.txt_mod.linear is a mod module, skipping
Quantizing weights:  63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                 | 720/1142 [00:06<00:03, 122.46it/s]
Warning: single_blocks.0.modulation.linear is a mod module, skipping
Quantizing weights:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                              | 733/1142 [00:06<00:03, 112.50it/s]
Warning: single_blocks.1.modulation.linear is a mod module, skipping
Quantizing weights:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                            | 745/1142 [00:06<00:05, 77.21it/s]
Warning: single_blocks.2.modulation.linear is a mod module, skipping
Quantizing weights:  66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                           | 755/1142 [00:06<00:05, 75.93it/s]
Warning: single_blocks.3.modulation.linear is a mod module, skipping
Quantizing weights:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                         | 765/1142 [00:06<00:05, 74.53it/s]
Warning: single_blocks.4.modulation.linear is a mod module, skipping
Quantizing weights:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 775/1142 [00:06<00:04, 73.90it/s]
Warning: single_blocks.5.modulation.linear is a mod module, skipping
Quantizing weights:  69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                     | 785/1142 [00:07<00:04, 73.37it/s]
Warning: single_blocks.6.modulation.linear is a mod module, skipping
Quantizing weights:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 795/1142 [00:07<00:04, 72.84it/s]
Warning: single_blocks.7.modulation.linear is a mod module, skipping
Quantizing weights:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 805/1142 [00:07<00:04, 70.32it/s]
Warning: single_blocks.8.modulation.linear is a mod module, skipping
Quantizing weights:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 815/1142 [00:07<00:04, 69.99it/s]
Warning: single_blocks.9.modulation.linear is a mod module, skipping
Quantizing weights:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                             | 825/1142 [00:07<00:04, 68.87it/s]
Warning: single_blocks.10.modulation.linear is a mod module, skipping
Quantizing weights:  73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 835/1142 [00:07<00:04, 69.19it/s]
Warning: single_blocks.11.modulation.linear is a mod module, skipping
Quantizing weights:  74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 845/1142 [00:07<00:04, 70.09it/s]
Warning: single_blocks.12.modulation.linear is a mod module, skipping
Quantizing weights:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                       | 855/1142 [00:08<00:04, 70.71it/s]
Warning: single_blocks.13.modulation.linear is a mod module, skipping
Quantizing weights:  76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                     | 865/1142 [00:08<00:03, 71.03it/s]
Warning: single_blocks.14.modulation.linear is a mod module, skipping
Quantizing weights:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                   | 875/1142 [00:08<00:03, 71.30it/s]
Warning: single_blocks.15.modulation.linear is a mod module, skipping
Quantizing weights:  77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 885/1142 [00:08<00:03, 70.39it/s]
Warning: single_blocks.16.modulation.linear is a mod module, skipping
Quantizing weights:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                               | 895/1142 [00:08<00:03, 70.88it/s]
Warning: single_blocks.17.modulation.linear is a mod module, skipping
Quantizing weights:  79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 905/1142 [00:08<00:03, 69.81it/s]
Warning: single_blocks.18.modulation.linear is a mod module, skipping
Quantizing weights:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 915/1142 [00:08<00:03, 69.29it/s]
Warning: single_blocks.19.modulation.linear is a mod module, skipping
Quantizing weights:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 925/1142 [00:09<00:03, 70.02it/s]
Warning: single_blocks.20.modulation.linear is a mod module, skipping
Quantizing weights:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 935/1142 [00:09<00:02, 70.36it/s]
Warning: single_blocks.21.modulation.linear is a mod module, skipping
Quantizing weights:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                      | 945/1142 [00:09<00:02, 70.89it/s]
Warning: single_blocks.22.modulation.linear is a mod module, skipping
Quantizing weights:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                    | 955/1142 [00:09<00:02, 71.29it/s]
Warning: single_blocks.23.modulation.linear is a mod module, skipping
Quantizing weights:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 965/1142 [00:09<00:02, 71.38it/s]
Warning: single_blocks.24.modulation.linear is a mod module, skipping
Quantizing weights:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 975/1142 [00:09<00:02, 71.15it/s]
Warning: single_blocks.25.modulation.linear is a mod module, skipping
Quantizing weights:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 985/1142 [00:09<00:02, 70.94it/s]
Warning: single_blocks.26.modulation.linear is a mod module, skipping
Quantizing weights:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 995/1142 [00:10<00:02, 66.85it/s]
Warning: single_blocks.27.modulation.linear is a mod module, skipping
Quantizing weights:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 1005/1142 [00:10<00:02, 67.93it/s]
Warning: single_blocks.28.modulation.linear is a mod module, skipping
Quantizing weights:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 1015/1142 [00:10<00:01, 69.59it/s]
Warning: single_blocks.29.modulation.linear is a mod module, skipping
Quantizing weights:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 1025/1142 [00:10<00:01, 70.12it/s]
Warning: single_blocks.30.modulation.linear is a mod module, skipping
Quantizing weights:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 1035/1142 [00:10<00:01, 69.59it/s]
Warning: single_blocks.31.modulation.linear is a mod module, skipping
Quantizing weights:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 1045/1142 [00:10<00:01, 68.31it/s]
Warning: single_blocks.32.modulation.linear is a mod module, skipping
Quantizing weights:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 1055/1142 [00:10<00:01, 68.45it/s]
Warning: single_blocks.33.modulation.linear is a mod module, skipping
Quantizing weights:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 1065/1142 [00:11<00:01, 69.14it/s]
Warning: single_blocks.34.modulation.linear is a mod module, skipping
Quantizing weights:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 1075/1142 [00:11<00:00, 68.30it/s]
Warning: single_blocks.35.modulation.linear is a mod module, skipping
Quantizing weights:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████           | 1085/1142 [00:11<00:00, 68.47it/s]
Warning: single_blocks.36.modulation.linear is a mod module, skipping
Quantizing weights:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 1095/1142 [00:11<00:00, 69.02it/s]
Warning: single_blocks.37.modulation.linear is a mod module, skipping
Quantizing weights:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 1105/1142 [00:11<00:00, 66.94it/s]
Warning: single_blocks.38.modulation.linear is a mod module, skipping
Quantizing weights:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 1115/1142 [00:11<00:00, 66.79it/s]
Warning: single_blocks.39.modulation.linear is a mod module, skipping
Warning: final_layer.linear is not in a block module, skipping
Warning: final_layer.adaLN_modulation.1 is a mod module, skipping
Quantizing weights: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1142/1142 [00:11<00:00, 96.51it/s]
========================= load vae =========================
2025-10-15 20:53:31.696 | INFO     | hymm_sp.vae:load_vae:47 - Loading 3D VAE model (884-16c-hy0801) from: weights/stdmodels/vae_3d/hyvae
WORLD SIZE:  1
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/vae/__init__.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute
arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpicklin
g. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loa
ded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(Path(vae_path) / "pytorch_model.pt", map_location=vae.device)
2025-10-15 20:53:39.188 | INFO     | hymm_sp.vae:load_vae:70 - VAE to dtype: torch.float16
========================= load llava =========================
2025-10-15 20:53:39.316 | INFO     | hymm_sp.text_encoder:load_text_encoder:29 - Loading text encoder model (llava-llama-3-8b) from: weights/stdmodels/llava-llama-3-8b-v1_1-transformers
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:43<00:00, 10.76s/it]
2025-10-15 20:54:28.687 | INFO     | hymm_sp.text_encoder:load_text_encoder:52 - Text encoder to dtype: torch.float16
2025-10-15 20:54:28.691 | INFO     | hymm_sp.text_encoder:load_tokenizer:67 - Loading tokenizer (llava-llama-3-8b) from: weights/stdmodels/llava-llama-3-8b-v1_1-transformers
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour,
 set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2025-10-15 20:54:29.112 | INFO     | hymm_sp.text_encoder:load_text_encoder:29 - Loading text encoder model (clipL) from: weights/stdmodels/openai_clip-vit-large-patch14
2025-10-15 20:54:29.462 | INFO     | hymm_sp.text_encoder:load_text_encoder:52 - Text encoder to dtype: torch.float16
2025-10-15 20:54:29.463 | INFO     | hymm_sp.text_encoder:load_tokenizer:67 - Loading tokenizer (clipL) from: weights/stdmodels/openai_clip-vit-large-patch14
load hunyuan model successful...
2025-10-15 20:54:29.824 | INFO     | __main__:main:126 - Enabled CPU offloading for transformer blocks
2025-10-15 20:54:29.824 | INFO     | __main__:main:132 - Prompt: A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky., Image Path asset/village.png
2025-10-15 20:54:35.537 | INFO     | __main__:main:246 - Generating segment 1/4 with action ID: w
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py:83: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  rays_dxo = torch.cross(rays_o, rays_d)                          # B, V, HW, 3
2025-10-15 20:54:38.385 | INFO     | hymm_sp.sample_inference:predict:641 -
                  size: (704, 1216)
          video_length: 34
                prompt: ['A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky.']
            neg_prompt: ['']
                  seed: 250160
           infer_steps: 50
      denoise_strength: 1.0
         use_deepcache: 1
              use_sage: False
           cpu_offload: True
 num_images_per_prompt: 1
        guidance_scale: 2.0
              n_tokens: 33440
            flow_shift: 5.0
                output: ./results_poor/
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
  0%|                                                                                                                                                                                                                                                             | 0/50 [00:00<?, ?it/s]
cpu_offload=True and                                 torch.Size([88, 152]) is large, split infer noise-pred
  0%|                                                                                                                                                                                                                                                             | 0/50 [01:12<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 298, in <module>
[rank0]:     main()
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 248, in main
[rank0]:     outputs = hunyuan_video_sampler.predict(
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py", line 644, in predict
[rank0]:     samples = self.pipeline(prompt=prompt,
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/diffusion/pipelines/pipeline_hunyuan_video_game.py", line 992, in __call__
[rank0]:     noise_pred_uncond = self.transformer(latent_model_input[:1],
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/hooks/hooks.py", line 148, in new_forward
[rank0]:     output = function_reference.forward(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 608, in forward
[rank0]:     img, txt = block(*double_block_args)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/hooks/hooks.py", line 148, in new_forward
[rank0]:     output = function_reference.forward(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 173, in forward
[rank0]:     attn = flash_attn_varlen_func(
[rank0]: NameError: name 'flash_attn_varlen_func' is not defined
[rank0]:[W1015 20:56:16.437450798 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finish
ed in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
E1015 20:56:19.538717 36748 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 36797) of binary: /home/ubuntu/miniconda3/envs/HYGameCraft/bin/python3.10
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
hymm_sp/sample_batch.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-15_20:56:19
  host      : ip-172-31-1-60.us-west-2.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 36797)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ cd ../
Hunyuan-GameCraft-1.0/ SageAttention/         lost+found/
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ cd ../SageAttention/
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/SageAttention$ pip install -e .
Obtaining file:///opt/dlami/nvme/SageAttention
  Preparing metadata (setup.py) ... done
Installing collected packages: sageattention
  DEPRECATION: Legacy editable install of sageattention==2.2.0 from file:///opt/dlami/nvme/SageAttention (setup.py develop) is deprecated. pip 25.3 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 6
4. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for sageattention
Successfully installed sageattention
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/SageAttention$ cd -
/opt/dlami/nvme/Hunyuan-GameCraft-1.0
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
models: cpu_offload=1, DISABLE_SP=1
TrtRunner or EngineFromBytes is not available, you can not use trt engine
text_encoder: cpu_offload=1
usage: sample_batch.py [-h] [--model MODEL] [--latent-channels LATENT_CHANNELS] [--rope-theta ROPE_THETA] [--vae VAE] [--vae-precision VAE_PRECISION] [--vae-tiling] [--text-encoder {clipL,llava-llama-3-8b}] [--text-encoder-precision {bf16,fp32,fp16}]
                       [--text-states-dim TEXT_STATES_DIM] [--text-len TEXT_LEN] [--tokenizer {clipL,llava-llama-3-8b}] [--text-encoder-infer-mode {encoder,decoder}] [--prompt-template-video {li-dit-encode-video}] [--hidden-state-skip-layer HIDDEN_STATE_SKIP_LAYER]
                       [--apply-final-norm] [--text-encoder-2 {clipL,llava-llama-3-8b}] [--text-encoder-precision-2 {bf16,fp32,fp16}] [--text-states-dim-2 TEXT_STATES_DIM_2] [--tokenizer-2 {clipL,llava-llama-3-8b}] [--text-len-2 TEXT_LEN_2]
                       [--text-projection {single_refiner,linear}] [--flow-shift-eval-video FLOW_SHIFT_EVAL_VIDEO] [--flow-reverse] [--flow-solver FLOW_SOLVER] [--use-linear-quadratic-schedule] [--linear-schedule-end LINEAR_SCHEDULE_END] [--precision {bf16,fp32,fp16}]
                       [--reproduce] [--ckpt CKPT] [--load-key {module,ema}] [--cpu-offload] [--use-fp8] [--video-size VIDEO_SIZE [VIDEO_SIZE ...]] [--sample-n-frames SAMPLE_N_FRAMES] [--infer-steps INFER_STEPS] [--val-disable-autocast] [--num-images NUM_IMAGES] [--seed SEED]
                       [--save-path-suffix SAVE_PATH_SUFFIX] [--prompt PROMPT] [--pos-prompt POS_PROMPT] [--neg-prompt NEG_PROMPT] [--add-pos-prompt ADD_POS_PROMPT] [--add-neg-prompt ADD_NEG_PROMPT] [--pad-face-size PAD_FACE_SIZE] [--image-path IMAGE_PATH]
                       [--save-path SAVE_PATH] [--input INPUT] [--item-name ITEM_NAME] [--cfg-scale CFG_SCALE] [--ip-cfg-scale IP_CFG_SCALE] [--use-deepcache USE_DEEPCACHE] [--use-sage] [--image-start] [--use-csv-pose] [--add-button] [--action-list ACTION_LIST [ACTION_LIST ...]]
                       [--action-speed-list ACTION_SPEED_LIST [ACTION_SPEED_LIST ...]] [--pose POSE]
sample_batch.py: error: unrecognized arguments: True
E1015 21:03:34.806710 58232 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 58270) of binary: /home/ubuntu/miniconda3/envs/HYGameCraft/bin/python3.10
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
hymm_sp/sample_batch.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-15_21:03:34
  host      : ip-172-31-1-60.us-west-2.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 58270)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ echo $DISABLE_SP

(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ export $DISABLE_SP=0
bash: export: `=0': not a valid identifier
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
^CW1015 21:06:18.662050 62892 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers
W1015 21:06:18.662603 62892 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 62920 closing signal SIGINT
Traceback (most recent call last):
  File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 8, in <module>
    import torchvision.transforms as transforms
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torchvision/models/__init__.py", line 2, in <module>
    from .convnext import *
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torchvision/models/convnext.py", line 8, in <module>
    from ..ops.misc import Conv2dNormActivation, Permute
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torchvision/ops/__init__.py", line 1, in <module>
    from ._register_onnx_ops import _register_custom_op
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torchvision/ops/_register_onnx_ops.py", line 5, in <module>
    from torch.onnx import symbolic_opset11 as opset11
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/__init__.py", line 71, in <module>
    from .utils import (
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/utils.py", line 31, in <module>
    from torch.onnx._internal import diagnostics, jit_utils, onnx_proto_utils, registration
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/__init__.py", line 1, in <module>
    from ._diagnostic import (
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/_diagnostic.py", line 11, in <module>
    from torch.onnx._internal.diagnostics import infra
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/__init__.py", line 1, in <module>
    from ._infra import (
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/_infra.py", line 11, in <module>
    from torch.onnx._internal.diagnostics.infra import formatter, sarif
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/formatter.py", line 10, in <module>
    from torch.onnx._internal.diagnostics.infra import sarif
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/sarif/__init__.py", line 22, in <module>
    from torch.onnx._internal.diagnostics.infra.sarif._external_properties import (
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/sarif/_external_properties.py", line 9, in <module>
    from torch.onnx._internal.diagnostics.infra.sarif import (
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/sarif/_result.py", line 9, in <module>
    from torch.onnx._internal.diagnostics.infra.sarif import (
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/onnx/_internal/diagnostics/infra/sarif/_result_provenance.py", line 16, in <module>
    class ResultProvenance(object):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/dataclasses.py", line 1178, in dataclass
    return wrap(cls)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/dataclasses.py", line 1169, in wrap
    return _process_class(cls, init, repr, eq, order, unsafe_hash,
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/dataclasses.py", line 1019, in _process_class
    _init_fn(all_init_fields,
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/dataclasses.py", line 574, in _init_fn
    return _create_fn('__init__',
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/dataclasses.py", line 434, in _create_fn
    exec(txt, globals, ns)
  File "<string>", line 1, in <module>
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 62892 got signal: 2

(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ ^C
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ export DISABLE_SP=0
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
models: cpu_offload=1, DISABLE_SP=1
TrtRunner or EngineFromBytes is not available, you can not use trt engine
text_encoder: cpu_offload=1
2025-10-15 21:06:36.719 | INFO     | __main__:main:85 - ********************
2025-10-15 21:06:36.822 | INFO     | __main__:main:91 - ++++++++++++++++++++
2025-10-15 21:06:36.822 | INFO     | __main__:main:96 - Generated videos will be saved to: ./results_poor/
2025-10-15 21:06:36.822 | INFO     | __main__:main:107 - Loading model from checkpoint: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 21:06:36.822 | INFO     | hymm_sp.inference:from_pretrained:62 - Got text-to-video model root path: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 21:06:36.822 | INFO     | hymm_sp.inference:from_pretrained:70 - Building model...
========================= build model =========================
^X^CW1015 21:06:40.130751 63357 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers
W1015 21:06:40.131212 63357 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 63400 closing signal SIGINT
[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 298, in <module>
[rank0]:     main()
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 108, in main
[rank0]:     hunyuan_video_sampler = HunyuanVideoSampler.from_pretrained(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/inference.py", line 75, in from_pretrained
[rank0]:     model = load_model(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/__init__.py", line 30, in load_model
[rank0]:     model = HYVideoDiffusionTransformer(
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/configuration_utils.py", line 693, in inner_init
[rank0]:     init(self, *args, **init_kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 459, in __init__
[rank0]:     [
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 460, in <listcomp>
[rank0]:     DoubleStreamBlock(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 71, in __init__
[rank0]:     self.img_mlp = MLP(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/mlp_layers.py", line 36, in __init__
[rank0]:     self.fc1 = linear_layer(in_channels, hidden_channels, bias=bias[0], **factory_kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 112, in __init__
[rank0]:     self.reset_parameters()
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 118, in reset_parameters
[rank0]:     init.kaiming_uniform_(self.weight, a=math.sqrt(5))
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/init.py", line 518, in kaiming_uniform_
[rank0]:     return tensor.uniform_(-bound, bound, generator=generator)
[rank0]: KeyboardInterrupt
[rank0]:[W1015 21:06:40.581724411 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finish
ed in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 63357 got signal: 2

(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ ^C
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
models: cpu_offload=1, DISABLE_SP=0
TrtRunner or EngineFromBytes is not available, you can not use trt engine
text_encoder: cpu_offload=1
2025-10-15 21:06:56.972 | INFO     | __main__:main:85 - ********************
2025-10-15 21:06:57.073 | INFO     | __main__:main:91 - ++++++++++++++++++++
2025-10-15 21:06:57.073 | INFO     | __main__:main:96 - Generated videos will be saved to: ./results_poor/
2025-10-15 21:06:57.073 | INFO     | __main__:main:107 - Loading model from checkpoint: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 21:06:57.073 | INFO     | hymm_sp.inference:from_pretrained:62 - Got text-to-video model root path: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 21:06:57.074 | INFO     | hymm_sp.inference:from_pretrained:70 - Building model...
========================= build model =========================
==================== load transformer to cpu
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/inference.py:167: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute ar
bitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling.
 Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loade
d file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(ckpt_path, map_location=lambda storage, loc: storage)
Quantizing weights:   0%|                                                                                                                                                                                                                                       | 0/1142 [00:00<?, ?it/s]
Warning: txt_in.input_embedder is not in a block module, skipping
Warning: txt_in.t_embedder.mlp.0 is not in a block module, skipping
Warning: txt_in.t_embedder.mlp.2 is not in a block module, skipping
Warning: txt_in.c_embedder.linear_1 is not in a block module, skipping
Warning: txt_in.c_embedder.linear_2 is not in a block module, skipping
Quantizing weights:   2%|█████                                                                                                                                                                                                                        | 26/1142 [00:00<00:05, 219.41it/s]
Warning: txt_in.individual_token_refiner.blocks.0.adaLN_modulation.1 is a mod module, skipping
Quantizing weights:   4%|█████████▎                                                                                                                                                                                                                   | 48/1142 [00:00<00:07, 138.86it/s]
Warning: txt_in.individual_token_refiner.blocks.1.adaLN_modulation.1 is a mod module, skipping
Warning: time_in.mlp.0 is not in a block module, skipping
Warning: time_in.mlp.2 is not in a block module, skipping
Warning: vector_in.in_layer is not in a block module, skipping
Warning: vector_in.out_layer is not in a block module, skipping
Warning: double_blocks.0.img_mod.linear is a mod module, skipping
Quantizing weights:   6%|██████████████▏                                                                                                                                                                                                              | 73/1142 [00:00<00:06, 174.15it/s]
Warning: double_blocks.0.txt_mod.linear is a mod module, skipping
Quantizing weights:   8%|█████████████████▉                                                                                                                                                                                                           | 93/1142 [00:00<00:07, 135.82it/s]
Warning: double_blocks.1.img_mod.linear is a mod module, skipping
Quantizing weights:  10%|████████████████████▉                                                                                                                                                                                                       | 109/1142 [00:00<00:07, 140.33it/s]
Warning: double_blocks.1.txt_mod.linear is a mod module, skipping
Quantizing weights:  11%|████████████████████████                                                                                                                                                                                                    | 125/1142 [00:00<00:08, 125.29it/s]
Warning: double_blocks.2.img_mod.linear is a mod module, skipping
Quantizing weights:  12%|██████████████████████████▊                                                                                                                                                                                                 | 139/1142 [00:01<00:08, 114.29it/s]
Warning: double_blocks.2.txt_mod.linear is a mod module, skipping
Quantizing weights:  14%|█████████████████████████████▊                                                                                                                                                                                              | 155/1142 [00:01<00:08, 112.66it/s]
Warning: double_blocks.3.img_mod.linear is a mod module, skipping
Quantizing weights:  15%|█████████████████████████████████▏                                                                                                                                                                                          | 172/1142 [00:01<00:08, 114.09it/s]
Warning: double_blocks.3.txt_mod.linear is a mod module, skipping
Quantizing weights:  16%|████████████████████████████████████▏                                                                                                                                                                                       | 188/1142 [00:01<00:08, 113.15it/s]
Warning: double_blocks.4.img_mod.linear is a mod module, skipping
Quantizing weights:  18%|███████████████████████████████████████▍                                                                                                                                                                                    | 205/1142 [00:01<00:08, 114.36it/s]
Warning: double_blocks.4.txt_mod.linear is a mod module, skipping
Quantizing weights:  19%|█████████████████████████████████████████▉                                                                                                                                                                                  | 218/1142 [00:01<00:07, 117.10it/s]
Warning: double_blocks.5.img_mod.linear is a mod module, skipping
Quantizing weights:  21%|██████████████████████████████████████████████▌                                                                                                                                                                             | 242/1142 [00:01<00:08, 105.57it/s]
Warning: double_blocks.5.txt_mod.linear is a mod module, skipping
Quantizing weights:  22%|████████████████████████████████████████████████▉                                                                                                                                                                           | 254/1142 [00:02<00:08, 109.13it/s]
Warning: double_blocks.6.img_mod.linear is a mod module, skipping
Quantizing weights:  24%|████████████████████████████████████████████████████▏                                                                                                                                                                       | 271/1142 [00:02<00:07, 110.72it/s]
Warning: double_blocks.6.txt_mod.linear is a mod module, skipping
Quantizing weights:  25%|███████████████████████████████████████████████████████▎                                                                                                                                                                    | 287/1142 [00:02<00:07, 110.45it/s]
Warning: double_blocks.7.img_mod.linear is a mod module, skipping
Quantizing weights:  27%|██████████████████████████████████████████████████████████▌                                                                                                                                                                 | 304/1142 [00:02<00:07, 111.79it/s]
Warning: double_blocks.7.txt_mod.linear is a mod module, skipping
Quantizing weights:  28%|█████████████████████████████████████████████████████████████▋                                                                                                                                                              | 320/1142 [00:02<00:07, 111.08it/s]
Warning: double_blocks.8.img_mod.linear is a mod module, skipping
Quantizing weights:  30%|████████████████████████████████████████████████████████████████▉                                                                                                                                                           | 337/1142 [00:02<00:07, 111.94it/s]
Warning: double_blocks.8.txt_mod.linear is a mod module, skipping
Quantizing weights:  31%|████████████████████████████████████████████████████████████████████                                                                                                                                                        | 353/1142 [00:02<00:07, 111.72it/s]
Warning: double_blocks.9.img_mod.linear is a mod module, skipping
Quantizing weights:  32%|███████████████████████████████████████████████████████████████████████▎                                                                                                                                                    | 370/1142 [00:03<00:06, 114.09it/s]
Warning: double_blocks.9.txt_mod.linear is a mod module, skipping
Quantizing weights:  34%|█████████████████████████████████████████████████████████████████████████▊                                                                                                                                                  | 383/1142 [00:03<00:06, 117.43it/s]
Warning: double_blocks.10.img_mod.linear is a mod module, skipping
Quantizing weights:  36%|██████████████████████████████████████████████████████████████████████████████▍                                                                                                                                             | 407/1142 [00:03<00:07, 100.05it/s]
Warning: double_blocks.10.txt_mod.linear is a mod module, skipping
Quantizing weights:  37%|█████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                          | 423/1142 [00:03<00:06, 105.10it/s]
Warning: double_blocks.11.img_mod.linear is a mod module, skipping
Quantizing weights:  39%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 440/1142 [00:03<00:06, 109.05it/s]
Warning: double_blocks.11.txt_mod.linear is a mod module, skipping
Quantizing weights:  40%|███████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                    | 456/1142 [00:03<00:06, 110.16it/s]
Warning: double_blocks.12.img_mod.linear is a mod module, skipping
Quantizing weights:  41%|███████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                 | 473/1142 [00:04<00:05, 112.09it/s]
Warning: double_blocks.12.txt_mod.linear is a mod module, skipping
Quantizing weights:  42%|█████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                              | 485/1142 [00:04<00:05, 111.86it/s]
Warning: double_blocks.13.img_mod.linear is a mod module, skipping
Quantizing weights:  44%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                           | 502/1142 [00:04<00:05, 114.16it/s]
Warning: double_blocks.13.txt_mod.linear is a mod module, skipping
Quantizing weights:  45%|███████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                        | 515/1142 [00:04<00:05, 117.39it/s]
Warning: double_blocks.14.img_mod.linear is a mod module, skipping
Quantizing weights:  47%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                    | 539/1142 [00:04<00:05, 100.68it/s]
Warning: double_blocks.14.txt_mod.linear is a mod module, skipping
Quantizing weights:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                 | 555/1142 [00:04<00:05, 104.51it/s]
Warning: double_blocks.15.img_mod.linear is a mod module, skipping
Quantizing weights:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 572/1142 [00:04<00:05, 109.40it/s]
Warning: double_blocks.15.txt_mod.linear is a mod module, skipping
Quantizing weights:  51%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                          | 588/1142 [00:05<00:04, 110.95it/s]
Warning: double_blocks.16.img_mod.linear is a mod module, skipping
Quantizing weights:  53%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                       | 605/1142 [00:05<00:04, 113.91it/s]
Warning: double_blocks.16.txt_mod.linear is a mod module, skipping
Quantizing weights:  54%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                    | 621/1142 [00:05<00:04, 114.19it/s]
Warning: double_blocks.17.img_mod.linear is a mod module, skipping
Quantizing weights:  56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                 | 638/1142 [00:05<00:04, 116.57it/s]
Warning: double_blocks.17.txt_mod.linear is a mod module, skipping
Quantizing weights:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 654/1142 [00:05<00:04, 115.29it/s]
Warning: double_blocks.18.img_mod.linear is a mod module, skipping
Quantizing weights:  59%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                          | 671/1142 [00:05<00:04, 116.60it/s]
Warning: double_blocks.18.txt_mod.linear is a mod module, skipping
Quantizing weights:  60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                       | 687/1142 [00:05<00:03, 117.51it/s]
Warning: double_blocks.19.img_mod.linear is a mod module, skipping
Quantizing weights:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                    | 704/1142 [00:06<00:03, 120.20it/s]
Warning: double_blocks.19.txt_mod.linear is a mod module, skipping
Quantizing weights:  63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                 | 720/1142 [00:06<00:03, 120.01it/s]
Warning: single_blocks.0.modulation.linear is a mod module, skipping
Quantizing weights:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                              | 733/1142 [00:06<00:03, 111.55it/s]
Warning: single_blocks.1.modulation.linear is a mod module, skipping
Quantizing weights:  65%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                            | 745/1142 [00:06<00:05, 77.36it/s]
Warning: single_blocks.2.modulation.linear is a mod module, skipping
Quantizing weights:  66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                           | 755/1142 [00:06<00:05, 72.91it/s]
Warning: single_blocks.3.modulation.linear is a mod module, skipping
Quantizing weights:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                         | 765/1142 [00:06<00:05, 68.93it/s]
Warning: single_blocks.4.modulation.linear is a mod module, skipping
Quantizing weights:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 775/1142 [00:07<00:05, 68.79it/s]
Warning: single_blocks.5.modulation.linear is a mod module, skipping
Quantizing weights:  69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                     | 785/1142 [00:07<00:05, 69.30it/s]
Warning: single_blocks.6.modulation.linear is a mod module, skipping
Quantizing weights:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 795/1142 [00:07<00:04, 69.67it/s]
Warning: single_blocks.7.modulation.linear is a mod module, skipping
Quantizing weights:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 805/1142 [00:07<00:04, 69.82it/s]
Warning: single_blocks.8.modulation.linear is a mod module, skipping
Quantizing weights:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 815/1142 [00:07<00:04, 70.03it/s]
Warning: single_blocks.9.modulation.linear is a mod module, skipping
Quantizing weights:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                             | 825/1142 [00:07<00:04, 70.18it/s]
Warning: single_blocks.10.modulation.linear is a mod module, skipping
Quantizing weights:  73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 835/1142 [00:07<00:04, 70.01it/s]
Warning: single_blocks.11.modulation.linear is a mod module, skipping
Quantizing weights:  74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                         | 845/1142 [00:08<00:04, 68.75it/s]
Warning: single_blocks.12.modulation.linear is a mod module, skipping
Quantizing weights:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                       | 855/1142 [00:08<00:04, 69.37it/s]
Warning: single_blocks.13.modulation.linear is a mod module, skipping
Quantizing weights:  76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                     | 865/1142 [00:08<00:03, 69.97it/s]
Warning: single_blocks.14.modulation.linear is a mod module, skipping
Quantizing weights:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                   | 875/1142 [00:08<00:03, 69.99it/s]
Warning: single_blocks.15.modulation.linear is a mod module, skipping
Quantizing weights:  77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                 | 885/1142 [00:08<00:03, 68.76it/s]
Warning: single_blocks.16.modulation.linear is a mod module, skipping
Quantizing weights:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                               | 895/1142 [00:08<00:03, 68.31it/s]
Warning: single_blocks.17.modulation.linear is a mod module, skipping
Quantizing weights:  79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 905/1142 [00:09<00:03, 65.48it/s]
Warning: single_blocks.18.modulation.linear is a mod module, skipping
Quantizing weights:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 915/1142 [00:09<00:03, 65.92it/s]
Warning: single_blocks.19.modulation.linear is a mod module, skipping
Quantizing weights:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 925/1142 [00:09<00:03, 63.80it/s]
Warning: single_blocks.20.modulation.linear is a mod module, skipping
Quantizing weights:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 935/1142 [00:09<00:03, 65.24it/s]
Warning: single_blocks.21.modulation.linear is a mod module, skipping
Quantizing weights:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                      | 945/1142 [00:09<00:02, 67.23it/s]
Warning: single_blocks.22.modulation.linear is a mod module, skipping
Quantizing weights:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                    | 955/1142 [00:09<00:02, 68.33it/s]
Warning: single_blocks.23.modulation.linear is a mod module, skipping
Quantizing weights:  85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 965/1142 [00:09<00:02, 69.11it/s]
Warning: single_blocks.24.modulation.linear is a mod module, skipping
Quantizing weights:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                | 975/1142 [00:10<00:02, 69.48it/s]
Warning: single_blocks.25.modulation.linear is a mod module, skipping
Quantizing weights:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                              | 985/1142 [00:10<00:02, 70.11it/s]
Warning: single_blocks.26.modulation.linear is a mod module, skipping
Quantizing weights:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                            | 995/1142 [00:10<00:02, 70.31it/s]
Warning: single_blocks.27.modulation.linear is a mod module, skipping
Quantizing weights:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 1005/1142 [00:10<00:01, 69.41it/s]
Warning: single_blocks.28.modulation.linear is a mod module, skipping
Quantizing weights:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 1015/1142 [00:10<00:01, 68.36it/s]
Warning: single_blocks.29.modulation.linear is a mod module, skipping
Quantizing weights:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 1025/1142 [00:10<00:01, 68.31it/s]
Warning: single_blocks.30.modulation.linear is a mod module, skipping
Quantizing weights:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                    | 1035/1142 [00:10<00:01, 68.83it/s]
Warning: single_blocks.31.modulation.linear is a mod module, skipping
Quantizing weights:  92%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                  | 1045/1142 [00:11<00:01, 68.99it/s]
Warning: single_blocks.32.modulation.linear is a mod module, skipping
Quantizing weights:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 1055/1142 [00:11<00:01, 69.54it/s]
Warning: single_blocks.33.modulation.linear is a mod module, skipping
Quantizing weights:  93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 1065/1142 [00:11<00:01, 69.11it/s]
Warning: single_blocks.34.modulation.linear is a mod module, skipping
Quantizing weights:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 1075/1142 [00:11<00:00, 67.26it/s]
Warning: single_blocks.35.modulation.linear is a mod module, skipping
Quantizing weights:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████           | 1085/1142 [00:11<00:00, 67.13it/s]
Warning: single_blocks.36.modulation.linear is a mod module, skipping
Quantizing weights:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 1095/1142 [00:11<00:00, 66.79it/s]
Warning: single_blocks.37.modulation.linear is a mod module, skipping
Quantizing weights:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊       | 1105/1142 [00:11<00:00, 67.86it/s]
Warning: single_blocks.38.modulation.linear is a mod module, skipping
Quantizing weights:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 1115/1142 [00:12<00:00, 68.47it/s]
Warning: single_blocks.39.modulation.linear is a mod module, skipping
Warning: final_layer.linear is not in a block module, skipping
Warning: final_layer.adaLN_modulation.1 is a mod module, skipping
Quantizing weights: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1142/1142 [00:12<00:00, 94.45it/s]
========================= load vae =========================
2025-10-15 21:09:53.558 | INFO     | hymm_sp.vae:load_vae:47 - Loading 3D VAE model (884-16c-hy0801) from: weights/stdmodels/vae_3d/hyvae
WORLD SIZE:  1
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/vae/__init__.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute
arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpicklin
g. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loa
ded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(Path(vae_path) / "pytorch_model.pt", map_location=vae.device)
2025-10-15 21:10:02.335 | INFO     | hymm_sp.vae:load_vae:70 - VAE to dtype: torch.float16
========================= load llava =========================
2025-10-15 21:10:02.463 | INFO     | hymm_sp.text_encoder:load_text_encoder:29 - Loading text encoder model (llava-llama-3-8b) from: weights/stdmodels/llava-llama-3-8b-v1_1-transformers
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:39<00:00,  9.88s/it]
2025-10-15 21:10:49.032 | INFO     | hymm_sp.text_encoder:load_text_encoder:52 - Text encoder to dtype: torch.float16
2025-10-15 21:10:49.039 | INFO     | hymm_sp.text_encoder:load_tokenizer:67 - Loading tokenizer (llava-llama-3-8b) from: weights/stdmodels/llava-llama-3-8b-v1_1-transformers
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour,
 set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2025-10-15 21:10:49.344 | INFO     | hymm_sp.text_encoder:load_text_encoder:29 - Loading text encoder model (clipL) from: weights/stdmodels/openai_clip-vit-large-patch14
2025-10-15 21:10:49.751 | INFO     | hymm_sp.text_encoder:load_text_encoder:52 - Text encoder to dtype: torch.float16
2025-10-15 21:10:49.752 | INFO     | hymm_sp.text_encoder:load_tokenizer:67 - Loading tokenizer (clipL) from: weights/stdmodels/openai_clip-vit-large-patch14
load hunyuan model successful...
2025-10-15 21:10:49.881 | INFO     | __main__:main:126 - Enabled CPU offloading for transformer blocks
2025-10-15 21:10:49.881 | INFO     | __main__:main:132 - Prompt: A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky., Image Path asset/village.png
2025-10-15 21:10:52.526 | INFO     | __main__:main:246 - Generating segment 1/4 with action ID: w
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py:83: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  rays_dxo = torch.cross(rays_o, rays_d)                          # B, V, HW, 3
2025-10-15 21:10:55.148 | INFO     | hymm_sp.sample_inference:predict:641 -
                  size: (704, 1216)
          video_length: 34
                prompt: ['A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky.']
            neg_prompt: ['']
                  seed: 250160
           infer_steps: 50
      denoise_strength: 1.0
         use_deepcache: 1
              use_sage: True
           cpu_offload: True
 num_images_per_prompt: 1
        guidance_scale: 2.0
              n_tokens: 33440
            flow_shift: 5.0
                output: ./results_poor/
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
  0%|                                                                                                                                                                                                                                                             | 0/50 [00:00<?, ?it/s]
cpu_offload=True and                                 torch.Size([88, 152]) is large, split infer noise-pred
  0%|                                                                                                                                                                                                                                                             | 0/50 [00:21<?, ?it/s]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 298, in <module>
[rank0]:     main()
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 248, in main
[rank0]:     outputs = hunyuan_video_sampler.predict(
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py", line 644, in predict
[rank0]:     samples = self.pipeline(prompt=prompt,
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/diffusion/pipelines/pipeline_hunyuan_video_game.py", line 992, in __call__
[rank0]:     noise_pred_uncond = self.transformer(latent_model_input[:1],
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/hooks/hooks.py", line 148, in new_forward
[rank0]:     output = function_reference.forward(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 608, in forward
[rank0]:     img, txt = block(*double_block_args)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/hooks/hooks.py", line 148, in new_forward
[rank0]:     output = function_reference.forward(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 184, in forward
[rank0]:     attn, _ = parallel_attention(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/parallel_states.py", line 356, in parallel_attention
[rank0]:     hidden_states = flash_attn_varlen_func(
[rank0]: NameError: name 'flash_attn_varlen_func' is not defined
[rank0]:[W1015 21:11:42.386020667 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finish
ed in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
E1015 21:11:45.161204 63910 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 63944) of binary: /home/ubuntu/miniconda3/envs/HYGameCraft/bin/python3.10
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
hymm_sp/sample_batch.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-15_21:11:45
  host      : ip-172-31-1-60.us-west-2.compute.internal
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 63944)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ python
Python 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from sageattention import sageattn'
  File "<stdin>", line 1
    from sageattention import sageattn'
                                      ^
SyntaxError: unterminated string literal (detected at line 1)
>>> from sageattention import sageattn
>>>
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
^CW1015 21:12:44.809123 73299 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers
W1015 21:12:44.809596 73299 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 73328 closing signal SIGINT
^CW1015 21:12:45.174323 73299 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 73328 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 73299 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 705, in run
    self._shutdown(e.sigval)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 365, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 572, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 909, in _close
    handler.proc.wait(time_to_wait)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/subprocess.py", line 1204, in wait
    return self._wait(timeout=timeout)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/subprocess.py", line 1932, in _wait
    time.sleep(delay)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 73299 got signal: 2

(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ ^C
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ sh scripts/1_gpu.sh
models: cpu_offload=0, DISABLE_SP=0
TrtRunner or EngineFromBytes is not available, you can not use trt engine
text_encoder: cpu_offload=1
2025-10-15 21:13:00.346 | INFO     | __main__:main:85 - ********************
2025-10-15 21:13:00.464 | INFO     | __main__:main:91 - ++++++++++++++++++++
2025-10-15 21:13:00.464 | INFO     | __main__:main:96 - Generated videos will be saved to: ./results_poor/
2025-10-15 21:13:00.464 | INFO     | __main__:main:107 - Loading model from checkpoint: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 21:13:00.464 | INFO     | hymm_sp.inference:from_pretrained:62 - Got text-to-video model root path: weights/gamecraft_models/mp_rank_00_model_states.pt
2025-10-15 21:13:00.464 | INFO     | hymm_sp.inference:from_pretrained:70 - Building model...
========================= build model =========================
==================== load transformer to cpu
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/inference.py:167: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute ar
bitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling.
 Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loade
d file. Please open an issue on GitHub for any issues related to this experimental feature.
  state_dict = torch.load(ckpt_path, map_location=lambda storage, loc: storage)
Quantizing weights:   0%|                                                                                                                                                                                                                                       | 0/1142 [00:00<?, ?it/s]
Warning: txt_in.input_embedder is not in a block module, skipping
Warning: txt_in.t_embedder.mlp.0 is not in a block module, skipping
Warning: txt_in.t_embedder.mlp.2 is not in a block module, skipping
Warning: txt_in.c_embedder.linear_1 is not in a block module, skipping
Warning: txt_in.c_embedder.linear_2 is not in a block module, skipping
Quantizing weights:   2%|█████                                                                                                                                                                                                                        | 26/1142 [00:00<00:04, 227.25it/s]
Warning: txt_in.individual_token_refiner.blocks.0.adaLN_modulation.1 is a mod module, skipping
Quantizing weights:   4%|█████████▍                                                                                                                                                                                                                   | 49/1142 [00:00<00:07, 136.95it/s]
Warning: txt_in.individual_token_refiner.blocks.1.adaLN_modulation.1 is a mod module, skipping
Warning: time_in.mlp.0 is not in a block module, skipping
Warning: time_in.mlp.2 is not in a block module, skipping
Warning: vector_in.in_layer is not in a block module, skipping
Warning: vector_in.out_layer is not in a block module, skipping
Warning: double_blocks.0.img_mod.linear is a mod module, skipping
Quantizing weights:   7%|██████████████▉                                                                                                                                                                                                              | 77/1142 [00:00<00:06, 157.92it/s]
Warning: double_blocks.0.txt_mod.linear is a mod module, skipping
Quantizing weights:   8%|██████████████████▏                                                                                                                                                                                                          | 94/1142 [00:00<00:07, 140.10it/s]
Warning: double_blocks.1.img_mod.linear is a mod module, skipping
Quantizing weights:  10%|█████████████████████▏                                                                                                                                                                                                      | 110/1142 [00:00<00:08, 128.59it/s]
Warning: double_blocks.1.txt_mod.linear is a mod module, skipping
Quantizing weights:  11%|███████████████████████▉                                                                                                                                                                                                    | 124/1142 [00:00<00:08, 126.85it/s]
Warning: double_blocks.2.img_mod.linear is a mod module, skipping
Quantizing weights:  12%|██████████████████████████▊                                                                                                                                                                                                 | 139/1142 [00:01<00:08, 117.21it/s]
Warning: double_blocks.2.txt_mod.linear is a mod module, skipping
Quantizing weights:  14%|█████████████████████████████▊                                                                                                                                                                                              | 155/1142 [00:01<00:08, 116.02it/s]
Warning: double_blocks.3.img_mod.linear is a mod module, skipping
Quantizing weights:  15%|█████████████████████████████████▏                                                                                                                                                                                          | 172/1142 [00:01<00:08, 116.75it/s]
Warning: double_blocks.3.txt_mod.linear is a mod module, skipping
Quantizing weights:  16%|███████████████████████████████████▋                                                                                                                                                                                        | 185/1142 [00:01<00:08, 119.27it/s]
Warning: double_blocks.4.img_mod.linear is a mod module, skipping
Quantizing weights:  18%|████████████████████████████████████████▍                                                                                                                                                                                   | 210/1142 [00:01<00:08, 106.43it/s]
Warning: double_blocks.4.txt_mod.linear is a mod module, skipping
Quantizing weights:  20%|███████████████████████████████████████████▎                                                                                                                                                                                | 225/1142 [00:01<00:08, 106.98it/s]
Warning: double_blocks.5.img_mod.linear is a mod module, skipping
Quantizing weights:  21%|██████████████████████████████████████████████▌                                                                                                                                                                             | 242/1142 [00:01<00:08, 111.98it/s]
Warning: double_blocks.5.txt_mod.linear is a mod module, skipping
Quantizing weights:  23%|█████████████████████████████████████████████████▋                                                                                                                                                                          | 258/1142 [00:02<00:07, 114.05it/s]
Warning: double_blocks.6.img_mod.linear is a mod module, skipping
Quantizing weights:  24%|████████████████████████████████████████████████████▉                                                                                                                                                                       | 275/1142 [00:02<00:07, 117.88it/s]
Warning: double_blocks.6.txt_mod.linear is a mod module, skipping
Quantizing weights:  25%|████████████████████████████████████████████████████████                                                                                                                                                                    | 291/1142 [00:02<00:07, 115.90it/s]
Warning: double_blocks.7.img_mod.linear is a mod module, skipping
Quantizing weights:  27%|███████████████████████████████████████████████████████████▎                                                                                                                                                                | 308/1142 [00:02<00:07, 115.40it/s]
Warning: double_blocks.7.txt_mod.linear is a mod module, skipping
Quantizing weights:  28%|█████████████████████████████████████████████████████████████▋                                                                                                                                                              | 320/1142 [00:02<00:07, 116.01it/s]
Warning: double_blocks.8.img_mod.linear is a mod module, skipping
Quantizing weights:  29%|████████████████████████████████████████████████████████████████▎                                                                                                                                                           | 334/1142 [00:02<00:06, 121.87it/s]
Warning: double_blocks.8.txt_mod.linear is a mod module, skipping
Quantizing weights:  31%|█████████████████████████████████████████████████████████████████████▏                                                                                                                                                      | 359/1142 [00:02<00:06, 112.70it/s]
Warning: double_blocks.9.img_mod.linear is a mod module, skipping
Quantizing weights:  33%|████████████████████████████████████████████████████████████████████████                                                                                                                                                    | 374/1142 [00:03<00:06, 110.51it/s]
Warning: double_blocks.9.txt_mod.linear is a mod module, skipping
Quantizing weights:  34%|███████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 390/1142 [00:03<00:06, 108.38it/s]
Warning: double_blocks.10.img_mod.linear is a mod module, skipping
Quantizing weights:  36%|██████████████████████████████████████████████████████████████████████████████▍                                                                                                                                             | 407/1142 [00:03<00:06, 112.41it/s]
Warning: double_blocks.10.txt_mod.linear is a mod module, skipping
Quantizing weights:  37%|█████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                          | 423/1142 [00:03<00:06, 112.32it/s]
Warning: double_blocks.11.img_mod.linear is a mod module, skipping
Quantizing weights:  39%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                       | 440/1142 [00:03<00:06, 115.45it/s]
Warning: double_blocks.11.txt_mod.linear is a mod module, skipping
Quantizing weights:  40%|███████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                    | 456/1142 [00:03<00:06, 113.47it/s]
Warning: double_blocks.12.img_mod.linear is a mod module, skipping
Quantizing weights:  41%|███████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                 | 473/1142 [00:03<00:05, 114.53it/s]
Warning: double_blocks.12.txt_mod.linear is a mod module, skipping
Quantizing weights:  43%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                             | 489/1142 [00:04<00:05, 114.85it/s]
Warning: double_blocks.13.img_mod.linear is a mod module, skipping
Quantizing weights:  44%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                          | 506/1142 [00:04<00:05, 117.16it/s]
Warning: double_blocks.13.txt_mod.linear is a mod module, skipping
Quantizing weights:  46%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                       | 522/1142 [00:04<00:05, 116.73it/s]
Warning: double_blocks.14.img_mod.linear is a mod module, skipping
Quantizing weights:  47%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                    | 539/1142 [00:04<00:05, 117.50it/s]
Warning: double_blocks.14.txt_mod.linear is a mod module, skipping
Quantizing weights:  49%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                 | 555/1142 [00:04<00:05, 116.91it/s]
Warning: double_blocks.15.img_mod.linear is a mod module, skipping
Quantizing weights:  50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                             | 572/1142 [00:04<00:04, 118.62it/s]
Warning: double_blocks.15.txt_mod.linear is a mod module, skipping
Quantizing weights:  51%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                          | 588/1142 [00:04<00:04, 117.04it/s]
Warning: double_blocks.16.img_mod.linear is a mod module, skipping
Quantizing weights:  53%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                       | 605/1142 [00:05<00:04, 116.39it/s]
Warning: double_blocks.16.txt_mod.linear is a mod module, skipping
Quantizing weights:  54%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                    | 621/1142 [00:05<00:04, 115.70it/s]
Warning: double_blocks.17.img_mod.linear is a mod module, skipping
Quantizing weights:  56%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                 | 638/1142 [00:05<00:04, 117.90it/s]
Warning: double_blocks.17.txt_mod.linear is a mod module, skipping
Quantizing weights:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 654/1142 [00:05<00:04, 115.83it/s]
Warning: double_blocks.18.img_mod.linear is a mod module, skipping
Quantizing weights:  58%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                           | 667/1142 [00:05<00:03, 118.99it/s]
Warning: double_blocks.18.txt_mod.linear is a mod module, skipping
Quantizing weights:  60%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                        | 683/1142 [00:05<00:03, 117.90it/s]
Warning: double_blocks.19.img_mod.linear is a mod module, skipping
Quantizing weights:  61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                     | 700/1142 [00:05<00:03, 119.68it/s]
Warning: double_blocks.19.txt_mod.linear is a mod module, skipping
Quantizing weights:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                | 728/1142 [00:06<00:04, 95.89it/s]
Warning: single_blocks.0.modulation.linear is a mod module, skipping
Quantizing weights:  65%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                              | 739/1142 [00:06<00:04, 89.24it/s]
Warning: single_blocks.1.modulation.linear is a mod module, skipping
Quantizing weights:  66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                            | 749/1142 [00:06<00:04, 83.33it/s]
Warning: single_blocks.2.modulation.linear is a mod module, skipping
Quantizing weights:  66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                          | 758/1142 [00:06<00:04, 77.57it/s]
Warning: single_blocks.3.modulation.linear is a mod module, skipping
Quantizing weights:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                        | 766/1142 [00:06<00:05, 71.35it/s]
Warning: single_blocks.4.modulation.linear is a mod module, skipping
Quantizing weights:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 775/1142 [00:06<00:05, 69.00it/s]
Warning: single_blocks.5.modulation.linear is a mod module, skipping
Quantizing weights:  69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                     | 785/1142 [00:07<00:05, 69.43it/s]
Warning: single_blocks.6.modulation.linear is a mod module, skipping
Quantizing weights:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 795/1142 [00:07<00:04, 69.72it/s]
Warning: single_blocks.7.modulation.linear is a mod module, skipping
Quantizing weights:  70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 805/1142 [00:07<00:04, 69.33it/s]
Warning: single_blocks.8.modulation.linear is a mod module, skipping
Quantizing weights:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 814/1142 [00:07<00:04, 73.81it/s]
Warning: single_blocks.9.modulation.linear is a mod module, skipping
Quantizing weights:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 824/1142 [00:07<00:04, 69.77it/s]
Warning: single_blocks.10.modulation.linear is a mod module, skipping
Quantizing weights:  73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                           | 834/1142 [00:07<00:04, 70.21it/s]
Warning: single_blocks.11.modulation.linear is a mod module, skipping
Quantizing weights:  74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                         | 844/1142 [00:07<00:04, 68.48it/s]
Warning: single_blocks.12.modulation.linear is a mod module, skipping
Quantizing weights:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                       | 854/1142 [00:08<00:04, 69.26it/s]
Warning: single_blocks.13.modulation.linear is a mod module, skipping
Quantizing weights:  76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                     | 864/1142 [00:08<00:04, 68.99it/s]
Warning: single_blocks.14.modulation.linear is a mod module, skipping
Quantizing weights:  77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                   | 874/1142 [00:08<00:03, 69.68it/s]
Warning: single_blocks.15.modulation.linear is a mod module, skipping
Quantizing weights:  77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 884/1142 [00:08<00:03, 69.64it/s]
Warning: single_blocks.16.modulation.linear is a mod module, skipping
Quantizing weights:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                | 894/1142 [00:08<00:03, 68.58it/s]
Warning: single_blocks.17.modulation.linear is a mod module, skipping
Quantizing weights:  79%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                              | 904/1142 [00:08<00:03, 68.47it/s]
Warning: single_blocks.18.modulation.linear is a mod module, skipping
Quantizing weights:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                            | 914/1142 [00:08<00:03, 69.23it/s]
Warning: single_blocks.19.modulation.linear is a mod module, skipping
Quantizing weights:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                          | 924/1142 [00:09<00:03, 69.62it/s]
Warning: single_blocks.20.modulation.linear is a mod module, skipping
Quantizing weights:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 934/1142 [00:09<00:02, 70.02it/s]
Warning: single_blocks.21.modulation.linear is a mod module, skipping
Quantizing weights:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 944/1142 [00:09<00:02, 70.15it/s]
Warning: single_blocks.22.modulation.linear is a mod module, skipping
Quantizing weights:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                    | 954/1142 [00:09<00:02, 69.43it/s]
Warning: single_blocks.23.modulation.linear is a mod module, skipping
Quantizing weights:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 964/1142 [00:09<00:02, 70.10it/s]
Warning: single_blocks.24.modulation.linear is a mod module, skipping
Quantizing weights:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 974/1142 [00:09<00:02, 68.65it/s]
Warning: single_blocks.25.modulation.linear is a mod module, skipping
Quantizing weights:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 984/1142 [00:09<00:02, 68.36it/s]
Warning: single_blocks.26.modulation.linear is a mod module, skipping
Quantizing weights:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                            | 994/1142 [00:10<00:02, 69.64it/s]
Warning: single_blocks.27.modulation.linear is a mod module, skipping
Quantizing weights:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                          | 1004/1142 [00:10<00:02, 68.44it/s]
Warning: single_blocks.28.modulation.linear is a mod module, skipping
Quantizing weights:  89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 1014/1142 [00:10<00:01, 68.03it/s]
Warning: single_blocks.29.modulation.linear is a mod module, skipping
Quantizing weights:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                      | 1024/1142 [00:10<00:01, 68.06it/s]
Warning: single_blocks.30.modulation.linear is a mod module, skipping
Quantizing weights:  91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 1034/1142 [00:10<00:01, 68.98it/s]
Warning: single_blocks.31.modulation.linear is a mod module, skipping
Quantizing weights:  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                   | 1044/1142 [00:10<00:01, 68.74it/s]
Warning: single_blocks.32.modulation.linear is a mod module, skipping
Quantizing weights:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 1054/1142 [00:11<00:01, 67.82it/s]
Warning: single_blocks.33.modulation.linear is a mod module, skipping
Quantizing weights:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 1064/1142 [00:11<00:01, 67.43it/s]
Warning: single_blocks.34.modulation.linear is a mod module, skipping
Quantizing weights:  94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 1074/1142 [00:11<00:00, 68.41it/s]
Warning: single_blocks.35.modulation.linear is a mod module, skipping
Quantizing weights:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊           | 1084/1142 [00:11<00:00, 69.23it/s]
Warning: single_blocks.36.modulation.linear is a mod module, skipping
Quantizing weights:  96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊         | 1094/1142 [00:11<00:00, 68.20it/s]
Warning: single_blocks.37.modulation.linear is a mod module, skipping
Quantizing weights:  97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋       | 1104/1142 [00:11<00:00, 69.15it/s]
Warning: single_blocks.38.modulation.linear is a mod module, skipping
Quantizing weights:  98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌     | 1114/1142 [00:11<00:00, 69.50it/s]
Warning: single_blocks.39.modulation.linear is a mod module, skipping
Warning: final_layer.linear is not in a block module, skipping
Warning: final_layer.adaLN_modulation.1 is a mod module, skipping
Quantizing weights: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1142/1142 [00:11<00:00, 95.73it/s]
========================= load vae =========================
2025-10-15 21:15:56.703 | INFO     | hymm_sp.vae:load_vae:47 - Loading 3D VAE model (884-16c-hy0801) from: weights/stdmodels/vae_3d/hyvae
WORLD SIZE:  1
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/vae/__init__.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute
arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpicklin
g. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loa
ded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(Path(vae_path) / "pytorch_model.pt", map_location=vae.device)
2025-10-15 21:16:04.816 | INFO     | hymm_sp.vae:load_vae:70 - VAE to dtype: torch.float16
========================= load llava =========================
2025-10-15 21:16:04.947 | INFO     | hymm_sp.text_encoder:load_text_encoder:29 - Loading text encoder model (llava-llama-3-8b) from: weights/stdmodels/llava-llama-3-8b-v1_1-transformers
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:39<00:00,  9.97s/it]
2025-10-15 21:16:51.555 | INFO     | hymm_sp.text_encoder:load_text_encoder:52 - Text encoder to dtype: torch.float16
2025-10-15 21:16:51.560 | INFO     | hymm_sp.text_encoder:load_tokenizer:67 - Loading tokenizer (llava-llama-3-8b) from: weights/stdmodels/llava-llama-3-8b-v1_1-transformers
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour,
 set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
2025-10-15 21:16:51.870 | INFO     | hymm_sp.text_encoder:load_text_encoder:29 - Loading text encoder model (clipL) from: weights/stdmodels/openai_clip-vit-large-patch14
2025-10-15 21:16:52.220 | INFO     | hymm_sp.text_encoder:load_text_encoder:52 - Text encoder to dtype: torch.float16
2025-10-15 21:16:52.223 | INFO     | hymm_sp.text_encoder:load_tokenizer:67 - Loading tokenizer (clipL) from: weights/stdmodels/openai_clip-vit-large-patch14
load hunyuan model successful...
2025-10-15 21:16:52.358 | INFO     | __main__:main:126 - Enabled CPU offloading for transformer blocks
2025-10-15 21:16:52.358 | INFO     | __main__:main:132 - Prompt: A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky., Image Path asset/village.png
2025-10-15 21:16:54.400 | INFO     | __main__:main:246 - Generating segment 1/4 with action ID: w
/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py:83: UserWarning: Using torch.cross without specifying the dim arg is deprecated.
Please either pass the dim explicitly or simply use torch.linalg.cross.
The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at ../aten/src/ATen/native/Cross.cpp:62.)
  rays_dxo = torch.cross(rays_o, rays_d)                          # B, V, HW, 3
2025-10-15 21:16:57.111 | INFO     | hymm_sp.sample_inference:predict:641 -
                  size: (704, 1216)
          video_length: 34
                prompt: ['A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky.']
            neg_prompt: ['']
                  seed: 250160
           infer_steps: 50
      denoise_strength: 1.0
         use_deepcache: 1
              use_sage: True
           cpu_offload: True
 num_images_per_prompt: 1
        guidance_scale: 2.0
              n_tokens: 33440
            flow_shift: 5.0
                output: ./results_poor/
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
  0%|                                                                                                                                                                                                                                                             | 0/50 [00:00<?, ?it/s]
cpu_offload=True and                                 torch.Size([88, 152]) is large, split infer noise-pred
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [22:40<00:00, 27.21s/it]
return_latents | TRUE
2025-10-15 21:40:14.044 | INFO     | hymm_sp.sample_inference:predict:714 - Success, time: 1396.932672739029
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-10-15 21:40:21.981 | INFO     | __main__:main:295 - Saved generated video to: ./results_poor//village.mp4
2025-10-15 21:40:21.982 | INFO     | __main__:main:246 - Generating segment 2/4 with action ID: a
2025-10-15 21:40:24.449 | INFO     | hymm_sp.sample_inference:predict:641 -
                  size: (704, 1216)
          video_length: 66
                prompt: ['A charming medieval village with cobblestone streets, thatched-roof houses, and vibrant flower gardens under a bright blue sky.']
            neg_prompt: ['']
                  seed: 250160
           infer_steps: 50
      denoise_strength: 1.0
         use_deepcache: 1
              use_sage: True
           cpu_offload: True
 num_images_per_prompt: 1
        guidance_scale: 2.0
              n_tokens: 60192
            flow_shift: 5.0
                output: ./results_poor/
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
encode prompt: move text_encoder to cuda
encode prompt successful: move text_encoder to cpu
  0%|                                                                                                                                                                                                                                                             | 0/50 [00:00<?, ?it/s]
cpu_offload=True and                                 torch.Size([88, 152]) is large, split infer noise-pred
  8%|███████████████████▌                                                                                                                                                                                                                                 | 4/50 [03:51<44:16, 57.76s/it]

 12%|█████████████████████████████▍                                                                                                                                                                                                                       | 6/50 [05:46<42:17, 57.68s/it]
^CW1015 21:46:46.748633 73651 site-packages/torch/distributed/elastic/agent/server/api.py:704] Received Signals.SIGINT death signal, shutting down workers
W1015 21:46:46.750545 73651 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 73705 closing signal SIGINT
 12%|█████████████████████████████▍                                                                                                                                                                                                                       | 6/50 [06:00<44:00, 60.02s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 298, in <module>
[rank0]:     if __name__ == "__main__":
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_batch.py", line 248, in main
[rank0]:     # Generate video segment with the current action
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/sample_inference.py", line 644, in predict
[rank0]:     samples = self.pipeline(prompt=prompt,
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/diffusion/pipelines/pipeline_hunyuan_video_game.py", line 992, in __call__
[rank0]:     noise_pred_uncond = self.transformer(latent_model_input[:1],
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/hooks/hooks.py", line 148, in new_forward
[rank0]:     output = function_reference.forward(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 620, in forward
[rank0]:     x = block(*single_block_args)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/diffusers/hooks/hooks.py", line 148, in new_forward
[rank0]:     output = function_reference.forward(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/models.py", line 353, in forward
[rank0]:     output = self.linear2(torch.cat((attn, self.mlp_act(mlp)), 2))
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/fp8_optimization.py", line 194, in forward
[rank0]:     output = fp8_gemm(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/fp8_optimization.py", line 148, in fp8_gemm
[rank0]:     output = triton_fp8_gemm(
[rank0]:   File "/opt/dlami/nvme/Hunyuan-GameCraft-1.0/hymm_sp/modules/fp8_optimization.py", line 110, in triton_fp8_gemm
[rank0]:     a_scale = a_scale.item()
[rank0]: KeyboardInterrupt
^CW1015 21:46:47.629655 73651 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 73705 closing signal SIGTERM
^CTraceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 696, in run
    result = self._invoke_run(role)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 855, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 73651 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 705, in run
    self._shutdown(e.sigval)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 365, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 572, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 909, in _close
    handler.proc.wait(time_to_wait)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/subprocess.py", line 1204, in wait
    return self._wait(timeout=timeout)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/subprocess.py", line 1932, in _wait
    time.sleep(delay)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 73651 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 710, in run
    self._shutdown()
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 365, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 572, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 909, in _close
    handler.proc.wait(time_to_wait)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/subprocess.py", line 1204, in wait
    return self._wait(timeout=timeout)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/subprocess.py", line 1932, in _wait
    time.sleep(delay)
  File "/home/ubuntu/miniconda3/envs/HYGameCraft/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 73651 got signal: 2
^C
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$ ^C
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$
(HYGameCraft) ubuntu@ip-172-31-1-60:/opt/dlami/nvme/Hunyuan-GameCraft-1.0$
